## 3.6 Batch Normalization(ë°°ì¹˜ ì •ê·œí™”)

> [Batch Normalization ì •ë¦¬](https://velog.io/@choiking10/Batch-Normalization%EC%97%90-%EB%8C%80%ED%95%B4%EC%84%9C-%EC%95%8C%EC%95%84%EB%B3%B4%EC%9E%90)

> [Batch Norm Explained Visually](https://towardsdatascience.com/batch-norm-explained-visually-how-it-works-and-why-neural-networks-need-it-b18919692739)

**Batch Normalization**(ë°°ì¹˜ ì •ê·œí™”)ëŠ” gradient vanishing ë° explosion ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì œì•ˆëœ ë°©ë²•ì´ë‹¤.

- **internal covariate shift**(ë‚´ë¶€ ê³µë³€ëŸ‰ ì´ë™)ì— ì´ˆì ì„ ë§ì¶˜ë‹¤.

> covariate(ê³µë³€ëŸ‰)ì´ë€ ë…ë¦½ë³€ìˆ˜ ì™¸ ì¢…ì†ë³€ìˆ˜ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆëŠ” ì¡ìŒì¸ì ë³€ëŸ‰ì„ ì˜ë¯¸í•œë‹¤.

> ë³´í†µ ì—°êµ¬ì—ì„œëŠ” ì—¬ëŸ¬ ë…ë¦½ë³€ìˆ˜ê°€ ì¢…ì†ë³€ìˆ˜ì— ì–¼ë§ˆë‚˜ ì˜í–¥ì„ ì£¼ëŠ”ì§€ í™•ì¸í•˜ë ¤ê³  í•˜ëŠ”ë°, ì¡ìŒì¸ìê°€ ìˆì„ ê²½ìš° ë…ë¦½ë³€ìˆ˜ì˜ ìˆœìˆ˜í•œ ì˜í–¥ë ¥ì„ ì•Œ ìˆ˜ ì—†ë‹¤.

---

### 3.6.1 shift and scaling, min-max scaling

ìš°ì„  Normalization(ì •ê·œí™”)ì˜ ë‘ ê°€ì§€ ëŒ€í‘œì ì¸ ë°©ì‹ì„ ë³´ì.

- **shift and scaling**

    ë°ì´í„°ë¥¼ í‰ê·  0, í‘œì¤€í¸ì°¨ 1ë¡œ ë³€í™˜í•œë‹¤. **Standardization**(í‘œì¤€í™”)ì´ë¼ê³ ë„ í•œë‹¤.

    > mean ê°’ì„ ë°”íƒ•ìœ¼ë¡œ shift, std ê°’ì„ ë°”íƒ•ìœ¼ë¡œ scalingí•œë‹¤.

$$ X' = {{X - \mu} \over \sigma} $$

- **min-max scaling**(ì •ê·œí™”)

    ë°ì´í„°ë¥¼ **0~1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ë³€í™˜**í•œë‹¤.

    > ì˜ˆë¥¼ ë“¤ì–´ ì´ë¯¸ì§€ ë°ì´í„°ê°€ í”½ì…€ ì •ë³´ë¥¼ 0\~255 ì‚¬ì´ ê°’ì„ ê°–ëŠ”ë‹¤ê³  í•˜ì. ì´ë¥¼ 255ë¡œ ë‚˜ëˆ„ë©´ 0.0\~1.0 ì‚¬ì´ ê°’ì„ ê°–ê²Œ ëœë‹¤.

$$ X' = {{X - X_{min}} \over {X_{max} - X_{min}}} $$

ì°¸ê³ ë¡œ batch normalizationì€ <U>activation distributionì„ standardizationí•˜ëŠ” ë°©ì‹</U>ì´ë‹¤.

---

### 3.6.2 covariate shift

ì•„ë˜ ê·¸ë¦¼ì€ **covariate shift**(ê³µë³€ëŸ‰ ì´ë™)ì„ ë‚˜íƒ€ë‚¸ë‹¤.

![covariate shift](images/covariate_shift.png)

- train/test dataset distributionì´ ë‹¤ë¥´ë‹¤.

í”íˆ í›ˆë ¨í•œ ëª¨ë¸ì„ test datasetìœ¼ë¡œ ì„±ëŠ¥ì„ ì¸¡ì •í–ˆì„ ë•Œ ì˜ ì•ˆë‚˜ì˜¤ëŠ” ê²½ìš° overfittingì„ ì˜ì‹¬í•˜ì§€ë§Œ, covariate shift ê´€ì ì—ì„œ ë³´ë©´ datasetì„ ìì²´ë¥¼ ì˜ëª» êµ¬ì„±í–ˆê¸° ë•Œë¬¸ì— ë°œìƒí–ˆì„ ìˆ˜ ìˆë‹¤.

ê°€ë ¹ ê²Œì„ ë Œë”ë§ìœ¼ë¡œ êµ¬ì„±ëœ ë„ë¡œì—ì„œ trainingì„ ìˆ˜í–‰í•œ ììœ¨ì£¼í–‰ ìë™ì°¨ê°€ ìˆë‹¤ê³  í•˜ì. train datasetì´ ë„ˆë¬´ ë‹¨ìˆœí•œ í…ìŠ¤ì²˜ë¡œë§Œ ì´ë£¨ì–´ì ¸ ìˆê¸° ë•Œë¬¸ì— ì‹¤ì œ ë³µì¡í•œ ë„ë¡œì—ì„œëŠ” ì˜ ë™ì‘í•˜ì§€ ì•Šì„ ê²ƒì´ë‹¤. ë˜í•œ ë‚®/ë°¤ì˜ ì°¨ì´ë§Œìœ¼ë¡œë„ ì •í™•ë„ ì°¨ì´ê°€ í¬ê²Œ ë‚  ìˆ˜ ìˆë‹¤.

---

### 3.6.3 internal covariate shift

> [internal covariate shift ì •ë¦¬](https://wegonnamakeit.tistory.com/47)

ì•„ë˜ ê·¸ë¦¼ì€ **Internal Covariate Shift**(ë‚´ë¶€ ê³µë³€ëŸ‰ ì´ë™)ì„ ë‚˜íƒ€ë‚¸ë‹¤.

![internal covariate shift](images/internal_covariate_shift.png)

- activation distributionì´ ë‹¬ë¼ì§„ë‹¤.

> ëŒ€í‘œì ì¸ ëŒ€ì•ˆìœ¼ë¡œëŠ” ReLU, careful initialized, small learning rate ë“±ì´ ìˆë‹¤.

ë§Œì•½ activationì„ standardizationí•œë‹¤ë©´, ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ activation ë¶„í¬ê°€ í‰ê·  0, í‘œì¤€í¸ì°¨ 1ë¡œ ë³€í™˜ëœë‹¤.(batch normalization ë ˆì´ì–´ë¥¼ ì ìš©)

![BN example](images/batch_normalization_ex.png)

> ê²°ë¡ ë¶€í„° ë³´ë©´, ì •ì‘ BNì´ ICSë¥¼ ì œê±°í•˜ëŠëƒ ë¬»ëŠ”ë‹¤ë©´ ê·¸ë ‡ì§€ëŠ” ì•Šë‹¤.

> ëŒ€ì‹  activationì´ ë™ì¼í•œ scaleì„ ê°–ê¸° ë•Œë¬¸ì—, gradient descentê°€ ì›í™œí•˜ê²Œ ìˆ˜ë ´ë  ìˆ˜ ìˆë‹¤.

---

### 3.6.4 batch, epoch

> [epoch vs batch vs mini-batch](https://www.baeldung.com/cs/epoch-vs-batch-vs-mini-batch)

- **epoch**

    ë„¤íŠ¸ì›Œí¬ê°€ ì „ì²´ training datasetì„ í•œ ë²ˆ í•™ìŠµí•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.(forward, backward passë¥¼ ëª¨ë‘ ê±°ì¹¨)

    - í•˜ì§€ë§Œ ë©”ëª¨ë¦¬ ì œì•½ìœ¼ë¡œ ëª¨ë“  í•™ìŠµ ë°ì´í„°ì…‹ì„ í•œ ë²ˆì— í•™ìŠµí•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, 1 epochì€ ì—¬ëŸ¬ iterationìœ¼ë¡œ ë‚˜ë‰˜ì–´ ìˆ˜í–‰ëœë‹¤. 
    
    - í•œ iterationì€ batch sizeë§Œí¼ì˜ dataë¥¼ ê°€ì§€ê³  í•™ìŠµí•œë‹¤.

- **batch**, **mini-batch**

    í•œ ë²ˆì˜ iterationì— ì‚¬ìš©ë˜ëŠ” sampleì˜ ìˆ˜ë¥¼ ì˜ë¯¸í•œë‹¤. ë‹¨, batch sizeë¥¼ ì „ì²´ training datasetìœ¼ë¡œ ì„¤ì •í•˜ëŠ”ê°€, ì•„ë‹ˆë©´ ì´ë³´ë‹¤ ì‘ì€ mini-batchë¡œ ì„¤ì •í•˜ëŠ”ê°€ì— ë”°ë¼ ë‹¤ë¥´ë‹¤.

    - batch

        ![batch](images/batch.jpg)

    - mini-batch

        ![mini-batch](images/mini_batch.jpg)

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ 1: iteration ìˆ˜ êµ¬í•˜ê¸°&nbsp;&nbsp;&nbsp;</span>

ì „ì²´ í•™ìŠµ ë°ì´í„°ì…‹ì˜ í¬ê¸°ëŠ” 2000ê°œì´ë‹¤. ì•„ë˜ ì¡°ê±´ì—ì„œ í•™ìŠµì—ì„œ ê°–ëŠ” ì´ iteration ìˆ˜ë¥¼ êµ¬í•˜ë¼.

- epochs = 20

- batch size = 500

### <span style='background-color: #C2B2B2; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ” í’€ì´&nbsp;&nbsp;&nbsp;</span>

ìš°ì„  batch sizeê°€ 500ì´ë¯€ë¡œ, 1 epochì€ 2000/500 = 4 iterationìœ¼ë¡œ ìˆ˜í–‰ëœë‹¤.

ê·¸ë¦¬ê³  epochì€ ì´ 20íšŒì´ë¯€ë¡œ 4 X 20 = 80 iterationì´ ìˆ˜í–‰ëœë‹¤.

---

### 3.6.5 Normalization Types

> [Group Normalization ë…¼ë¬¸](https://arxiv.org/abs/1803.08494)

ê·¸ëŸ°ë° normalizationë„ ì—¬ëŸ¬ ì „ëµì´ ìˆë‹¤.

![normalization](images/normalization.png)

ì´ì¤‘ì—ì„œë„ batch normalizationì„ ì‚¬ìš©í•˜ëŠ” ì´ì ì€, batchì˜ ëª¨ë“  sampleì´ ê°–ëŠ” ë¶„í¬ê°€ ê· ì¼í•´ì§„ë‹¤ëŠ” ì ì´ë‹¤. 

![standardization](images/standardization.png)

- í•˜ì§€ë§Œ ì˜¤ì§ batch sizeê°€ í° ê²½ìš°ì—ë§Œ ì˜ ë™ì‘í•œë‹¤.

- batch sizeê°€ ì‘ì€ ê²½ìš°, batch normalizationì€ ì—­ìœ¼ë¡œ ì„±ëŠ¥ì„ ë–¨ì–´ëœ¨ë¦´ ìˆ˜ ìˆë‹¤.

> ì´ì™¸ ë¬´ì‘ìœ„ dropoutì€ distributionì— ì˜í–¥ì„ ë§ì´ ë¯¸ì¹  ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, ë™ì¼í•œ networkì—ì„œ BNê³¼ í•¨ê»˜ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²ƒì´ ì¢‹ë‹¤.

í•˜ì§€ë§Œ ì´ëŸ¬í•œ ë³€í˜•ì€ ì§€ë‚œ layerì—ì„œ ì–»ì€ í‘œí˜„ë ¥ì„ ê°ì†Œì‹œí‚¬ ìˆ˜ ìˆë‹¤. íŠ¹íˆ ëª¨ë“  dataë¥¼ ë‹¨ìˆœíˆ í‰ê·  0, í‘œì¤€í¸ì°¨ 1ë¡œ ë³€í˜•ì‹œí‚¤ëŠ” **whitening** ë°©ì‹ì„ ì ìš©í•˜ë©´ ì´ëŸ° ë¬¸ì œê°€ ë°œìƒí•˜ê²Œ ëœë‹¤.

![whitening](images/whitening.png)

---

### 3.6.6 In-layer normalization

Batch Normalizationì„ ìˆ˜í–‰í•˜ëŠ” intermediate layerëŠ” activation functionì„ ì ìš©í•˜ê¸° ì „/í›„ ì¤‘ ì–´ë””ì— ì—°ê²°í•˜ëŠ”ê°€ì— ë”°ë¼ ë‘ ê°€ì§€ë¡œ ë‚˜ë‰œë‹¤.

- activation layer ì´í›„ ì—°ê²°

- activation layer ì´ì „ ì—°ê²°: inputì— linear transformì„ ì ìš©í•œ í›„, í•˜ì§€ë§Œ activation functionì„ ì ìš©í•˜ê¸° ì „ì— BNì„ ìˆ˜í–‰í•œë‹¤.

ì´ì „,ì´í›„ ì—°ê²°ì— ë”°ë¥¸ ì„±ëŠ¥ ì°¨ì´ëŠ” ì˜ê²¬ì´ ê°ˆë¦¬ì§€ë§Œ, activation ì´ì „ì— BNì„ ìˆ˜í–‰í•˜ëŠ” ê²½ìš°ê°€ ë³´í¸ì ì´ë‹¤. ë‹¤ë§Œ activation ì´ì „ì— BNì„ ìˆ˜í–‰í•˜ëŠ” ê²½ìš°, ì•„ë˜ì™€ ê°™ì€ ì´ìœ ë¡œ non-linearityê°€ ê°ì†Œí•  ìˆ˜ ìˆë‹¤.

![in-layer normalization](images/in-layer_normalization.png)

- ì˜ˆë¥¼ ë“¤ì–´ normalizationì„ ì ìš©í•œ ë’¤, ì˜¤ë¥¸ìª½ê³¼ ê°™ì€ tanh activationì„ ê±°ì¹œë‹¤ê³  í•˜ì.

- ì´ ê²½ìš°, activationì˜ ì…ë ¥ì€ ëŒ€ë¶€ë¶„ ê·¸ë¦¼ì˜ ë¹¨ê°„ìƒ‰ ë°•ìŠ¤ì— í•´ë‹¹ë˜ëŠ” ê°’ì´ ë  ê²ƒì´ë‹¤. ê·¸ëŸ°ë° **ì´ ë¹¨ê°„ìƒ‰ ë°•ìŠ¤ëŠ” ê±°ì˜ linearí•œ ì„±ì§ˆì„ ë¤ë‹¤.**

ë”°ë¼ì„œ ì´ëŸ¬í•œ non-linearityì˜ ê°ì†Œë¥¼ ë§‰ê¸° ìœ„í•´ ì ì ˆí•œ scale, shift íŒ¨ëŸ¬ë¯¸í„°ë¥¼ ê°–ëŠ”ë°, ì´ê²ƒì´ $\gamma$ , $\beta$ ì´ë‹¤.

$$ y_i \leftarrow {\gamma} \hat{x_i} + \beta $$

- $\hat{x_i}$ : normalization í›„, activation layerì˜ ì…ë ¥ì´ ë˜ëŠ” ê°’

ìœ„ ë‘ íŒ¨ëŸ¬ë¯¸í„°ëŠ” training ê³¼ì •ì—ì„œ í•™ìŠµëœë‹¤.(**learable parameter**) 

![BN algorithm](images/batch_normalization_algorithm.png)

ëª¨ë¸ì„ mini-batch ë‹¨ìœ„ë¡œ í•™ìŠµí•˜ê¸° ë•Œë¬¸ì—, íŒ¨ëŸ¬ë¯¸í„°ë„ mini-batch ë‹¨ìœ„ë¡œ ëª¨ë‘ ê°€ì§€ê²Œ ëœë‹¤. ê·¸ë¦¬ê³  ì¶”ë¡  ë‹¨ê³„ì—ì„œëŠ” $n$ ê°œì˜ mini-batchë¥¼ ê°€ì§€ê³ , ì¶”ë¡ ì— ì‚¬ìš©í•  í‰ê· , í‘œì¤€í¸ì°¨ë¥¼ êµ¬í•˜ê²Œ ëœë‹¤.

> layerê°€ normalizationëœ ê°’ì„ ì…ì¶œë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì—, ì¶”ë¡  ì‹œì—ë„ normalizationì´ ì ìš©ë˜ì–´ì•¼ ì œëŒ€ë¡œ ëœ ì¶”ë¡ ì´ ê°€ëŠ¥í•˜ë‹¤.

ì´ì²˜ëŸ¼ ì¶”ë¡  ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•˜ëŠ” parameterë¥¼ **moving average**(ì´ë™ í‰ê· )ì„ í†µí•´ êµ¬í•œë‹¤.

---

### 3.6.7 Inference with Batch Normalization

ì¶”ë¡ (inference) ë‹¨ê³„ì—ì„œëŠ” ì´ëŸ¬í•œ mini-batchì˜ í‰ê· , í‘œì¤€í¸ì°¨ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤.

ë”°ë¼ì„œ ì¶”ë¡  ì‹œ ì‚¬ìš©í•  í‰ê· , í‘œì¤€í¸ì°¨ë¥¼ training setì˜ í‰ê· , í‘œì¤€í¸ì°¨ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•í•˜ì—¬ ë„ì¶œí•œë‹¤. ì´ë ‡ê²Œ ë„ì¶œí•œ parameterë¥¼ **moving mean**, **moving standard deviation**ë¼ê³  í•˜ë©°, non-learnable parameterë¡œ Batch Norm layerì˜ **state** ì¼ë¶€ë¡œì¨ ì €ì¥ëœë‹¤.

> ì•„ë˜ ì˜ˆì‹œëŠ” activation í›„ BNì„ ì ìš©í•˜ëŠ” ê²½ìš°ì´ë‹¤.

![BN state](images/batch_norm_state.png)

- ê° Batch Norm layerëŠ” ê³ ìœ í•œ $\beta$ , $\gamma$ , moving mean, moving standard deviationë¥¼ ê°–ëŠ”ë‹¤.

ì´ì œ moving averageë¥¼ ì–´ë–»ê²Œ êµ¬í•˜ëŠ”ì§€ ë³´ì.

![moving average](images/moving_average.png)

```math
\hat{\mu} \leftarrow \alpha \hat{\mu} + (1- \alpha) {\hat{\mu}}_{\beta}^{(i)}
```

```math
\hat{\sigma} \leftarrow \alpha \hat{\sigma} + (1- \alpha) {\hat{\sigma}}_{\beta}^{(i)}
```

- $\alpha$ : moving averageì˜ decay factor(momentum). ì£¼ë¡œ 1ì— ê°€ê¹Œìš´ 0.9, 0.99, 0.999ë¥¼ ì‚¬ìš©í•œë‹¤.

> moving average ë°©ì‹ì´ ì•„ë‹ˆë¼ ëª¨ì§‘ë‹¨ ì¶”ì •ë°©ì‹ì„ ì‚¬ìš©í•  ìˆ˜ë„ ìˆì§€ë§Œ, ì´ ê²½ìš° stateë¡œ ëª¨ë“  $\beta$ , $\gamma$ ë¥¼ ê¸°ì–µí•´ ë‘¬ì•¼ í•˜ë¯€ë¡œ ë¹„íš¨ìœ¨ì ì´ë‹¤.

---

### 3.6.8 Batch Normalization in CNN

![CNN BN](images/CNN_BN.png)

CNN(Convolutional Neural Network)ì— BNì„ ì ìš© ì‹œ ê¸°ì¡´ê³¼ ëª‡ ê°€ì§€ ì°¨ì´ì ì´ ìˆë‹¤.

- $b$ : $Wx+b$ ì—ì„œ $\beta$ ê°€ $b$ (bias) ì—­í• ì„ ëŒ€ì²´í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— $b$ ë¥¼ ì œê±°í•œë‹¤.

- channelë³„ë¡œ BNì„ ìˆ˜í–‰í•œë‹¤.

  - ì¦‰, Batch, Height, Width ì •ë³´ë¥¼ ê°–ëŠ” tensorë³„ë¡œ í‰ê· ê³¼ ë¶„ì‚°ì„ êµ¬í•œë‹¤.

    > feed-forward (dense) layerëŠ” ìì²´ì ì¸ ê°€ì¤‘ì¹˜ë¥¼ ê°–ëŠ” ë‰´ëŸ° ë‹¨ìœ„ë¡œ BNì„ ì ìš©í•œë‹¤.

    > í•˜ì§€ë§Œ CNNì€ ê°™ì€ transformation(filter)ì´ inputì˜ volumeë³„ë¡œ ì ìš©ë˜ê³ , ì¶œë ¥ìœ¼ë¡œ í•˜ë‚˜ì˜ channelì„ êµ¬ì„±ëœë‹¤.(filter ê°œìˆ˜ = ì¶œë ¥ ì±„ë„ í¬ê¸°)

---