# ì–•ì€ ì‹ ê²½ë§ì„ ì´ìš©í•œ ê¸°ê³„ í•™ìŠµ

## 2.1 ì†Œê°œ

ëŒ€ì²´ë¡œ ì „í†µì ì¸ ê¸°ê³„ í•™ìŠµì€ optimization(ìµœì í™”)ì™€ backpropagation(ê²½ì‚¬ í•˜ê°•ë²•)ì„ ì´ìš©í•´ì„œ parameterized modelì„ í•™ìŠµí•œë‹¤. ì´ëŸ° modelì˜ ì˜ˆì‹œë¡œ linear regression, SVM, logistic regression, dimention reduction, matrix decomposition(í–‰ë ¬ ì¸ìˆ˜ë¶„í•´) ë“±ì´ ìˆë‹¤.

![í¼ì…‰íŠ¸ë¡ ì˜ ì—¬ëŸ¬ ë³€í˜•](images/perceptron_machine_learning.png)

ì´ë²ˆ ì¥ì€ ê¸°ê³„ í•™ìŠµì˜ ì•„ì£¼ ë‹¤ì–‘í•œ optimization ì¤‘ì‹¬ ë°©ë²•ë“¤ì€ layerê°€ 1~2ê°œë¿ì¸ ì•„ì£¼ ë‹¨ìˆœí•œ ì‹ ê²½ë§ êµ¬ì¡°ë¡œ í‰ë‚´ ë‚¼ ìˆ˜ ìˆìŒì„ ë³´ì¼ ê²ƒì´ë‹¤.

---

## 2.2 binary classification(ì´ì§„ ë¶„ë¥˜) modelì„ ìœ„í•œ ì‹ ê²½ë§ êµ¬ì¡°

ì´ë²ˆ ì ˆ ì „ë°˜ì€ input nodeê°€ dê°œ, output nodeê°€ 1ê°œì¸ í¼ì…‰íŠ¸ë¡ ì„ ì‚¬ìš©í•œë‹¤.

- $\bar{W} = (w_1, ... , w_d)$ : weight

- bias í•­ì€ êµ³ì´ ë‘ì§€ ì•Šê³ , ê°’ì´ 1ì¸ ê°€ì§œ input nodeë¥¼ í•˜ë‚˜ ì¶”ê°€í•´ì„œ, ê·¸ ê³„ìˆ˜ë¥¼ bias í•­ì²˜ëŸ¼ ì“¸ ê²ƒì´ë‹¤.

### 2.2.1 í¼ì…‰íŠ¸ë¡  ë‹¤ì‹œ ë³´ê¸°

ì˜ˆì¸¡ê°’ì€ ë‹¤ìŒê³¼ ê°™ì´ ì“¸ ìˆ˜ ìˆë‹¤.

$$ \hat{y_i} = sgn(\overline{W} \cdot \overline{X_i}) $$

- $(\overline{X_i}, y_i)$ : í•˜ë‚˜ì˜ í›ˆë ¨ ê²¬ë³¸

í¼ì…‰íŠ¸ë¡  ê°±ì‹  ê³µì‹ì€ ë‹¤ìŒê³¼ ê°™ì•˜ë‹¤.(regularizationì´ ì ìš©ëœ)

$$ \overline{W} \Leftarrow \overline{W}(1-\alpha\lambda) + \alpha(y_i - \hat{y_i}) \overline{X_i} $$

backpropagationì˜ ê°±ì‹ ì´ ì´ì²˜ëŸ¼ ì˜¤ì°¨ì— ë¹„ë¡€í•˜ëŠ” ê²½ìš°, $(y_i - \hat{y_i})^2$ ê°™ì€ ì œê³± ì†ì‹¤í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ìì—°ìŠ¤ëŸ½ë‹¤. ê·¸ëŸ°ë° ì§€ê¸ˆ ì¶œë ¥ì€ ì´ì‚°ê°’ì´ê¸° ë•Œë¬¸ì—, loss functionì˜ ê°’ë§ˆì € ì´ì‚°ì ì´ê²Œ ëœë‹¤. ì§€ì ë§ˆë‹¤ ê³„ë‹¨ ëª¨ì–‘ì„ ê·¸ë¦¬ê¸° ë•Œë¬¸ì— ë¯¸ë¶„í•  ìˆ˜ ì—†ë‹¤.

ê·¸ë˜ì„œ 1.2.1.1ì ˆì—ì„œ ì •ë¦¬í–ˆë“¯ ë¯¸ë¶„ ê°€ëŠ¥í•œ smoothed surrogate loss function(í‰í™œí™”ëœ ëŒ€ë¦¬ í•¨ìˆ˜)ë¥¼ ëŒ€ì‹  ì‚¬ìš©í•œë‹¤ê³  ì •ë¦¬í–ˆë‹¤. ì´ë•Œ ì˜¤ë¶„ë¥˜ëœ í›ˆë ¨ ê²¬ë³¸ ( $y_i \hat{y_i}$ < 0 )ì—ì„œë§Œ weight ê°±ì‹ ì´ ì¼ì–´ë‚¬ë‹¤. 

ê°±ì‹  ê³µì‹ì„ indicator function(ì§€ì‹œí•¨ìˆ˜: ì¸ìˆ˜ë¡œ ì£¼ì–´ì§„ ì¡°ê±´ì´ ì„±ë¦½í•˜ë©´ 1, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ 0) $I(\cdot) \in \lbrace 0, 1 \rbrace$ ì„ ì‚¬ìš©í•´ì„œ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.

$$ \overline{W} \Leftarrow \overline{W}(1-\alpha\lambda) + \alpha y_i \overline{X_i}[I(y_i \hat{y_i} < 0)] $$

- ì˜¤ë¶„ë¥˜ëœ ê²¬ë³¸ì¼ ê²½ìš° $(y_i - \hat{y_i}) / 2$ ê°€ ì˜ˆì¸¡ê°’ì´ë¯€ë¡œ, ìœ„ì™€ ê°™ì´ $(y_i - \hat{y_i})$ ì˜¤ì°¨ ë¶€ë¶„ì„ ìˆ˜ì •í–ˆë‹¤.(ê³„ìˆ˜ 2ëŠ” learning rateë¡œ í¡ìˆ˜)

ië²ˆì§¸ í›ˆë ¨ ê²¬ë³¸ì— ëŒ€í•œ loss functionì€ ë‹¤ìŒê³¼ ê°™ì•˜ë‹¤. ë˜í•œ ì´ loss functionì„ **perceptron criterion**(í¼ì…‰íŠ¸ë¡  íŒì •ê¸°ì¤€)ì´ë¼ê³  ë¶ˆë €ë‹¤.

$$ L_i = \max \lbrace 0, -y_i(\overline{W} \cdot \overline{X_i}) \rbrace $$

---

### 2.2.2 least-squares regression(ìµœì†Œì œê³± íšŒê·€)

least-squares regressionì—ì„œ train dataëŠ” nê°œì˜ ì„œë¡œ ë‹¤ë¥¸ í›ˆë ¨ ê²¬ë³¸ $(\overline{X_1}, y_1)...(\overline{X_n}, y_n)$ ìœ¼ë¡œ ì´ë£¨ì–´ì§„ë‹¤. 

- ì—¬ê¸°ì„œ ê° $\overline{X_i}$ data pointëŠ” dì°¨ì› í‘œí˜„ì´ê³ , $y_i$ ëŠ” real(ì‹¤ìˆ˜) target(ëª©í‘¯ê°’)ì´ë‹¤. ì‹¤ìˆ˜ì´ê¸° ë•Œë¬¸ì— regression ë¬¸ì œê°€ ëœë‹¤.

ië²ˆì§¸ í›ˆë ¨ ê²¬ë³¸ì— ëŒ€í•œ loss functionì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$ L_i = e_i^2 = (y_i - \hat{y_i})^2 $$

backpropagation ê°±ì‹  ê³µì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$ \overline{W} \Leftarrow \overline{W} + \alpha e_i \overline{X} $$

ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ë„ ìˆë‹¤.

$$ \overline{W} \Leftarrow \overline{W} + \alpha (y_i - \hat{y_i}) \overline{X} $$

regularizationì„ ì ìš©í•  ìˆ˜ë„ ìˆë‹¤.

$$ \overline{W} \Leftarrow \overline{W}(1 - \alpha \cdot \lambda) + \alpha (y_i - \hat{y_i}) \overline{X} $$

ì´ ê°±ì‹  ê³µì‹ì€ 2.2.1ì˜ perceptron criterionê³¼ ë§¤ìš° ë¹„ìŠ·í•˜ë‹¤. ê·¸ëŸ¬ë‚˜ ë‘ ê°±ì‹  ê³µì‹ì€ ì™„ì „íˆ ê°™ì€ ê²ƒì€ ì•„ë‹ˆë‹¤.( $\hat{y_i}$ ê³„ì‚° ë°©ì‹ì´ ë‹¤ë¥´ê¸° ë•Œë¬¸)

ê·¸ë ‡ë‹¤ë©´ binary targetì— ì ìš©í•˜ë©´ ì–´ë–¨ê¹Œ? ì´ ê²½ìš°ëŠ” least-squares classification(ìµœì†Œì œê³± ë¶„ë¥˜) ë¬¸ì œê°€ ëœë‹¤. ì´ ê²½ìš° perceptron criterion(í¼ì…‰íŠ¸ë¡  íŒì • ê¸°ì¤€)ê³¼ ê²‰ë³´ê¸°ì— ë™ì¼í•œë°, í¼ì…‰íŠ¸ë¡  ì•Œê³ ë¦¬ì¦˜ê³¼ ê²°ê³¼ê°€ ê°™ì§€ëŠ” ì•Šë‹¤.

ê·¸ ì´ìœ ëŠ” least-squares classificationì˜ 'ì‹¤ìˆ«ê°’' í›ˆë ¨ ì˜¤ì°¨ $(y_i - \hat{y_i})$ ì™€, í¼ì…‰íŠ¸ë¡  'ì •ìˆ˜' ì˜¤ì°¨ $(y_i - \hat{y_i})$ ì˜ ê³„ì‚° ë°©ì‹ì´ ì™„ì „íˆ ë‹¤ë¥´ê¸° ë•Œë¬¸ì´ë‹¤. (ë°”ë¡œ ì•„ë˜ 2.2.2.1ì ˆì—ì„œ ì„¤ëª…)

> ì´ëŸ° least-squares classification(ìµœì†Œì œê³± ë¶„ë¥˜)ë¥¼ Widrow-Hoff learning(ìœ„ë“œë¡œ-í˜¸í”„ í•™ìŠµ)ì´ë¼ê³  í•œë‹¤.

---

### 2.2.2.1 Widrow-Hoff learning(ìœ„ë“œë¡œ-í˜¸í”„ í•™ìŠµ)

ê¸°ì¡´ì˜ least-squares regressionì„ binary targetì— ì ìš©í•˜ê³ ì í•˜ëŠ” ì‹œë„ì—ì„œ íƒ„ìƒí–ˆë‹¤. 

Widrow-Hoff learningì€ ë¯¸ì§€ì˜ ì‹œí—˜ ê²¬ë³¸ì˜ ì‹¤ìˆ˜ ì˜ˆì¸¡ê°’ì„ sign functionì„ ì´ìš©í•´ì„œ binary targetìœ¼ë¡œ ë³€í™˜í•˜ê¸´ í•˜ì§€ë§Œ, <U>í›ˆë ¨ ê²¬ë³¸ê³¼ì˜ ì˜¤ì°¨ëŠ” ì‹¤ìˆ˜ ì˜ˆì¸¡ê°’ì„ ì§ì ‘ ì‚¬ìš©í•´ì„œ ê³„ì‚°í•œë‹¤.</U> 

> í¼ì…‰íŠ¸ë¡ ì˜ ê²½ìš° ì˜¤ì°¨ëŠ” í•­ìƒ {-2, +2}ì— ì†í•˜ì§€ë§Œ, Widrow-Hoff learningì˜ ê²½ìš° $\hat{y_i}$ ê°€ sign function ì—†ì´ $\overline{W} \cdot \overline{X_i}$ ë¡œ ê²°ì •ë˜ê¸° ë•Œë¬¸ì— ì˜¤ì°¨ê°€ ì„ì˜ì˜ ì‹¤ìˆ«ê°’ì´ë‹¤.

ì´ ë•Œë¬¸ì— positive ë¶€ë¥˜ì— ì†í•˜ëŠ” data pointê°€ $\overline{W} \cdot \overline{X_i} > 1$ ì¸ ê²½ìš°ì—ì„œ ì°¨ì´ë¥¼ ë³´ì¼ ìˆ˜ ìˆë‹¤. í¼ì…‰íŠ¸ë¡ ì€ ë²Œì ì´ ê°€í•´ì§€ì§€ ì•Šê² ì§€ë§Œ, Widrow-Hoffì—ì„œëŠ” ì‹¤ìˆ˜ ì˜ˆì¸¡ê°’ ì˜¤ì°¨ì´ê¸° ë•Œë¬¸ì— ë²Œì ì´ ê°€í•´ì§€ê²Œ ëœë‹¤.

<U>ì„±ê³¼ê°€ ë„ˆë¬´ ì¢‹ì€ pointì—ë„ ë¶€ë‹¹í•˜ê²Œ ë²Œì ì´ ê°€í•´ì§€ëŠ” ê²ƒ</U>ì´ ì´ Widrow-Hoff learningì˜ ë‹¨ì ì´ë‹¤.

ì´ ë°©ë²•ì˜ loss functionì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$ L_i = (y_i - \hat{y_i})^2 = y_i^2(y_i - \hat{y_i})^2 = (1-\hat{y_i}y_i)^2 $$

- $y_i^2 = 1$ ì´ë¯€ë¡œ loss functionì— ê³±í•˜ëŠ” ê²ƒìœ¼ë¡œ ëª¨ì–‘ì„ ë°”ê¾¼ ê²ƒì´ë‹¤.

> ìœ„ loss functionì„ 'ë„ˆë¬´ ì¢‹ì€ ì„±ê³¼'ì— ë²Œì ì„ ë¶€ì—¬í•˜ì§€ ì•Šë„ë¡ ìˆ˜ì •í•˜ëŠ” í•œ ë°©ë²•ì„ ì ìš©í•˜ë©´ SVMì˜ loss functionì´ ëœë‹¤.

---

### 2.2.3 logistic regression

logistic regression(ë¡œì§€ìŠ¤í‹± íšŒê·€)ëŠ” ê²¬ë³¸ë“¤ì„ í™•ë¥ ì— ê·¼ê±°í•´ì„œ ë¶„ë¥˜í•˜ëŠ” í•˜ë‚˜ì˜ í™•ë¥  ëª¨í˜•ì´ë‹¤. ê° í›ˆë ¨ ê²¬ë³¸ì˜ 'ì •ë‹µì— í•´ë‹¹í•  í™•ë¥ (ì˜ˆì¸¡ê°’)'ì„ ìµœëŒ€í•œ í¬ê²Œ ë§Œë“œëŠ” ê²ƒì´ ëª©í‘œë‹¤. ì´ëŸ° optimization(ìµœì í™”) ëª©í‘œëŠ” **maximum-likelihood estimation**(ìµœëŒ€ê°€ëŠ¥ë„ ì¶”ì •)ì´ë¼ëŠ” ê¸°ë²•ì„ ì´ìš©í•´ì„œ ë‹¬ì„±í•  ìˆ˜ ìˆë‹¤.

output nodeì˜ <U>loss functionì€ ìŒì˜ **log-likelihood**(ë¡œê·¸ê°€ëŠ¥ë„)</U>ì´ê³ , Widrow-Hoffì˜ ì œê³± ì˜¤ì°¨ ëŒ€ì‹  ì´ loss functionì„ ì“°ëŠ” ê²ƒì´ ë°”ë¡œ logistic regressionì´ë‹¤.

> output layerì˜ activation functionìœ¼ë¡œëŠ” sigmoidë¥¼ ì£¼ë¡œ ì‚¬ìš©í•œë‹¤.

ë‹¤ìŒì€ logistic regressionì˜ outputì„ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•œ ê²ƒì´ë‹¤.

$$ \hat{y_i} = P(y_i = 1) = {{1} \over {1 + exp(-\overline{W} \cdot \overline{X_i})}} $$

- input data: $\lbrace (\overline{X_1}, y_1),  (\overline{X_2}, y_2),...,  (\overline{X_n}, y_n) \rbrace$

  - $d$ ì°¨ì› feature vector $\overline{X_i}$  ì™€, targetì¸ $y_i \in \lbrace -1, +1 \rbrace$ ìœ¼ë¡œ êµ¬ì„±ëœ í›ˆë ¨ ê²¬ë³¸ nê°œ ì§‘í•©

- weight: $\overline{W} = (w_1, ..., w_d)$

- activation function: sigmoid

ì—¬ê¸°ì„œ ì‹ ê²½ë§ì€ $P(y_i = 1) > 0.5$ ì¸ ë¶€ë¥˜, ì¦‰ í•´ë‹¹ ì˜ˆì¸¡ í™•ë¥ ì´ 0.5ë³´ë‹¤ í° ë¶€ë¥˜ë¥¼ ê²°ê³¼(ìµœì¢… ì˜ˆì¸¡ê°’)ìœ¼ë¡œ ì¶œë ¥í•œë‹¤.

> ì´ë•Œ ë¶„ëª¨ì˜ ë²”ìœ„ì— ì§‘ì¤‘í•˜ì. ë§Œì•½ $\overline{W} \cdot \overline{X_i}=0$ ë¼ë©´ $P(y_i = 1) = 0.5$ ì´ë‹¤. $\overline{W} \cdot \overline{X_i} > 0$ ì´ë©´ 0.5ë¥¼ ë„˜ê²Œ ëœë‹¤.

> ë”°ë¼ì„œ ì‚¬ì‹¤ìƒ $\overline{W} \cdot \overline{X_i}$ ì˜ **ë¶€í˜¸ì— ë”°ë¼ì„œ íŒë‹¨í•˜ëŠ” ê²ƒê³¼ ë‹¤ë¦„ì´ ì—†ë‹¤.**

ì–‘ì„±(positive) ê²¬ë³¸ì—ì„œ ì˜ˆì¸¡ê°’ $P(y_i = 1)$ ì˜ í™•ë¥ ì€ ìµœëŒ€í™”í•´ì•¼ í•˜ë©°, ìŒì„±(negative) ê²¬ë³¸ì—ì„œëŠ” ì˜ˆì¸¡ê°’ $P(y_i = -1)$ ì˜ í™•ë¥ ì€ ìµœì†Œí™”í•´ì•¼ í•œë‹¤.

ë‹¤ì‹œ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

- $y_i = 1$ ì¸ (positive) ê²¬ë³¸ì—ì„œëŠ” ì˜ˆì¸¡ê°’ $\hat{y_i}$ ë¥¼ ìµœëŒ€í™”

- $y_i = -1$ ì¸ (negative) ê²¬ë³¸ì—ì„œëŠ” ì˜ˆì¸¡ê°’ $1- \hat{y_i}$ ì„ ìµœëŒ€í™”í•´ì•¼ í•œë‹¤.

ì´ ë‘˜ì„ í†µí•©í•´ì„œ í‘œí˜„í•œ ìˆ˜ì‹, ì¦‰ ìµœëŒ€í™”í•´ì•¼ í•˜ëŠ” ê²ƒì„ í‘œí˜„í•˜ë©´ ë°”ë¡œ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$ | {{y_i} \over {2}} - {1 \over 2} + \hat{y_i} | $$

ê·¸ë¦¬ê³  ì´ <U>ìµœëŒ€í™”í•´ì•¼ í•˜ëŠ” ê°’ì„ ëª¨ë‘ ê³±í•œ ê°’ì„ ìµœëŒ€í™”</U>í•˜ë©´, ê²°ê³¼ì ìœ¼ë¡œ ê°€ëŠ¥ë„ $\mathcal{L}$ ì´ ìµœëŒ€í™”ëœë‹¤.

$$ \mathcal{L} = \prod_{i=1}^N | {{y_i} \over {2}} - {1 \over 2} + \hat{y_i} | $$

ì´ì œ loss functionì„ $L_i = -\log(\mathcal{L}) = \sum_{i=1}^{n} {-\log(| {{y_i} \over {2}} - {1 \over 2} + \hat{y_i} |)}$

- ì—¬ê¸°ì„œ $i$ ë²ˆì§¸ í›ˆë ¨ ê²¬ë³¸ì˜ loss functionì„ $L_i = -\log(| {{y_i} \over {2}} - {1 \over 2} + \hat{y_i} |)$ ë¼ê³  í•˜ë©´ ë” ê°„ë‹¨íˆ ì“¸ ìˆ˜ ìˆë‹¤.

- <U>logë¥¼ ì ìš©í•˜ëŠ” ê²ƒìœ¼ë¡œ ê³±ì´ ì•„ë‹Œ sum ê³„ì‚° í˜•íƒœ</U>ë¡œ ë°”ë€Œì—ˆë‹¤.

í™•ë¥ ì„ ë‹¤ë£¨ëŠ” backpropagation ê°±ì‹ ì—ì„œëŠ” ì´ë ‡ê²Œ <U>sum í˜•íƒœì˜ loss functionì´ ë” í¸ë¦¬</U>í•˜ë‹¤. 

$i$ ë²ˆì§¸ loss function $L_i$ ì„ $\overline{W}$ ë¡œ í¸ë¯¸ë¶„í•˜ì—¬ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. (ì—¬ê¸°ì„œ $\overline{W}$ ì™€ ê´€ë ¨ì´ ìˆëŠ” ë³€ìˆ˜ê°€ $\hat{y_i}$ ì¸ ê²ƒì„ ìœ ì˜í•˜ê³  ë³´ë©´ ì‰½ë‹¤.)

> ${d \over dx}|f(x)| = (\mathrm{sgn} \, f(x)){d \over dx} f(x) = {|f(x)| \over {f(x)}}{d \over dx} f(x)$ :  modulus(ì ˆëŒ“ê°’)ì´ ì ìš©ëœ $|f(x)|$ ì˜ ë¯¸ë¶„ì€ ì‚¬ì‹¤ìƒ ë‹¨ìˆœíˆ $f(x)$ ê°’ì˜ ë¶€í˜¸ ì—¬ë¶€ì— ë”°ë¼ +1, -1ì„ $f'(x)$ ì— ê³±í•œ ê²ƒ

> ${d \over dx}[log_{a}(u)] = {1 \over \ln{a}} \cdot {1 \over u}u'$

> ${d \over dx}{1 \over {1+ exp({-x})}} = {-{(-exp(-x))} \over {(1+exp(-x))^2}} = {{1} \over {1+exp(-x)}} \cdot {{exp(-x)} \over {1+exp(-x)}} = f(x)(1-f(x)) $

$$ {\partial{L_i} \over {\partial \overline{W}}} = - {{sgn({{y_i} \over {2}} - {1 \over 2} + \hat{y_i})} \over {|{{y_i} \over {2}} - {1 \over 2} + \hat{y_i}|}} \cdot {{\partial \hat{y_i}} \over {\partial \overline{W}}} $$

$$ = - {{sgn({{y_i} \over {2}} - {1 \over 2} + \hat{y_i})} \over {|{{y_i} \over {2}} - {1 \over 2} + \hat{y_i}|}} \cdot {{\overline{X_i}} \over {1+ exp(- \overline{W} \cdot \overline{X_i})}} \cdot { 1 \over {1+ exp(\overline{W} \cdot \overline{X_i})}} $$

$$ ë§Œì¼ \, y_i = 1 ì´ë©´ \quad - {{\overline{X_i}} \over {1+exp({\overline{W} \cdot \overline{X_i}})}} $$

$$ ë§Œì¼ \, y_i = -1 ì´ë©´ \quad {{\overline{X_i}} \over {1+exp({-\overline{W} \cdot \overline{X_i}})}} $$

ìœ„ ì‹ì„ $y_i \in \lbrace -1, +1 \rbrace$ ì„ ì´ìš©í•´ ì¢€ ë” ê°„ë‹¨íˆ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$ {\partial{L_i} \over {\partial \overline{W}}} = - {{y_i\overline{X_i}} \over {1+ exp(y_i\overline{W} \cdot \overline{X_i})}} $$

> $-$ \[ $(\overline{X_i}, y_i)$ ë¥¼ ì˜¤ë¶„ë¥˜í•  í™•ë¥  \]( $y_i \overline{X_i}$ ) 

ì´ìƒì— ê¸°ì´ˆí•´ì„œ logistic regressionì˜ backpropagation ê°±ì‹  ê³µì‹ì„ ì„¸ìš°ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$ \overline{W} \Leftarrow \overline{W}(1 - \alpha \lambda) + \alpha {{y_i \overline{X_i}} \over {1+exp[y_i (\overline{W} \cdot \overline{X_i})]}} $$

ì—¬ê¸°ì„œ ê¸°ì–µí•´ ë‘˜ ê´€ì ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

- perceptron, Widrow-Hoff algorithm: 'mistake(ì°©ì˜¤)ì˜ magnitude(í¬ê¸°)'ë¥¼ ì´ìš©í•´ì„œ ê°±ì‹ 

- logistic regression: 'mistake **í™•ë¥ **'ì„ ì´ìš©í•´ì„œ ê°±ì‹ 

---

### 2.2.3.1 ë‹¤ë¥¸ ì¢…ë¥˜ì˜ activation functionê³¼ loss functionì„ ì´ìš©í•œ êµ¬í˜„

ì‚¬ì‹¤ ë‹¤ë¥¸ ì¢…ë¥˜ì˜ activation functionê³¼ loss functionì„ ì´ìš©í•´ì„œë„ logistic regression modelì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤.

ê°€ë ¹ identity functionì„ ì´ìš©í•´ì„œ ì¶œë ¥ $\hat{y_i} \in (- \infty, +\infty)$ ë¥¼ ì–»ê³ , ê±°ê¸°ì— ë‹¤ìŒê³¼ ê°™ì€ loss functionì„ ì ìš©í•  ìˆ˜ë„ ìˆë‹¤.

$$ L_i = \log (1+ exp(-y_i \cdot \hat{y_i})) $$

ì´ modelì—ì„œ ìµœì¢… ì˜ˆì¸¡ê°’(output)ì€ $y_i$ ì— sign function(ë¶€í˜¸ í•¨ìˆ˜)ë¥¼ ì ìš©í•œ ê²ƒì´ë‹¤. 

---

## 2.2.4 Support Vector Machine(SVM)

Support Vector Machine(SVM, ì§€ì§€ ë²¡í„° ê¸°ê³„)ì˜ loss functionì€, logistic regressionì˜ loss functionê³¼ ë°€ì ‘í•œ ê´€ë ¨ì´ ìˆë‹¤. ë‹¨, <U>ë§¤ë„ëŸ¬ìš´ loss functionì´ ì•„ë‹Œ **hinge-loss**(ê²½ì²© ì†ì‹¤)ì„ ì‚¬ìš©</U>í•œë‹¤.

SVMì˜ ì‹ ê²½ë§ êµ¬ì¡°ëŠ” least-squares classification(Widrow-Hoff) êµ¬ì¡°ì™€ ë™ì¼í•˜ë‹¤. <U>ì£¼ëœ ì°¨ì´ëŠ” ë°”ë¡œ loss function</U>ì´ë‹¤.

> Widrow-Hoffì—ì„œëŠ” í›ˆë ¨ì  $\overline{X_i}$ ì˜ ì˜ˆì¸¡ê°’ $\hat{y_i}$ ë¥¼ $\overline{W} \cdot \overline{X}$ ì— identity functionì„ ì ìš©í•´ì„œ êµ¬í–ˆë‹¤. ( $\hat{y_i} = \overline{W} \cdot \overline{X}$ )

> ë˜í•œ Widrow-Hoffì˜ loss functionì€ ì‹¤ìˆ«ê°’ $(1-y_i \hat{y_i})^2$ ì˜€ë‹¤.

SVMì—ì„œ $i$ ë²ˆì§¸ í›ˆë ¨ ê²¬ë³¸ì˜ loss function $L_i$ ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•œë‹¤. (ì´ loss functionì´ ë°”ë¡œ hinge-lossë‹¤.)

$$ L_i = max \lbrace 0, 1-y_i \hat{y_i} \rbrace $$

ì´ loss function, ì¦‰ hinge-lossì˜ í•µì‹¬ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

- positive ê²¬ë³¸ì€ ê·¸ ê°’ì´ '1 ë¯¸ë§Œ'ì¼ ë•Œë§Œ ë²Œì ì´ ê°€í•´ì§„ë‹¤.

- negative ê²¬ë³¸ì€ ê·¸ ê°’ì´ '-1 ì´ˆê³¼'ì¼ ë•Œë§Œ ë²Œì ì´ ê°€í•´ì§„ë‹¤.

- ë‘ ê²½ìš° ëª¨ë‘ ë²Œì ì€ ì„ í˜•ì´ë˜, í•´ë‹¹ threshold(ë¬¸í„±) ê°’,(1 ë˜ëŠ” -1)ì„ ë„˜ìœ¼ë©´ ì¦‰ì‹œ í‰í‰í•´ì§„ë‹¤.

> Widrow-Hoffê°€ ì˜ˆì¸¡ê°’ì´ 'targetê³¼ ë‹¤ë¥¼ ë•Œë§ˆë‹¤' ë²Œì ì´ ë¶€ì—¬ëœ ê²ƒê³¼ ë¹„êµí•´ì„œ, ì´ëŸ¬í•œ íŠ¹ì„±ì€ SVMì˜ ì¥ì ì´ ëœë‹¤.

---

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ–Œ ë¹„êµ: 4ê°€ì§€ ë°©ë²•ì˜ loss function&nbsp;&nbsp;&nbsp;</span>

ì•„ë˜ ê·¸ë¦¼ì„ ë³´ë©´ Widrow-Hoff, logistic regression, SVMì˜ loss functionì„ ë¹„êµí•˜ëŠ” ë° ë„ì›€ì´ ë  ê²ƒì´ë‹¤. ê·¸ë˜í”„ëŠ” ì—¬ëŸ¬ $\hat{y_i} = \overline{W} \cdot \overline{X_i}$ ì´ë©°, positiveí•œ( targetì€ $+1$ ) ê²¬ë³¸ì˜ ì†ì‹¤ ê°’ì„ ë‚˜íƒ€ë‚´ê³  ìˆë‹¤. (ë˜í•œ perceptronì€ surrogate loss function ë²„ì „ì„ í‘œì‹œí–ˆë‹¤.)

![loss function ë¹„êµ](images/loss_function_compare.png)

- logistic regression: $\overline{W} \cdot \overline{X_i}$ ê°€ <U>+1ì„ ë„˜ìœ¼ë©´ loss functionì€ ì ì  ëŠë¦¬ê²Œ ê°ì†Œ</U>í•œë‹¤.

- SVM: hinge-lossëŠ” <U>+1ì„ ë„˜ì ì™„ì „íˆ í‰í‰</U>í•´ì§„ë‹¤. 

  - ì¦‰, ì˜¤ë¶„ë¥˜ëœ pointì™€ ê²°ì • ê²½ê³„ì„  $\overline{W} \cdot \overline{X} = 0$ ì— ê°€ê¹Œìš´ pointë§Œ ë²Œì ì„ ë°›ëŠ”ë‹¤.

- perceptron: perceptron criterionì€ <U>hinge-lossì™€ ëª¨ì–‘ì€ ê°™ì§€ë§Œ, ì™¼ìª½ìœ¼ë¡œ í•œ ë‹¨ìœ„ í‰í–‰ì´ë™</U>í•œ ëª¨ì–‘ì´ë‹¤. 

- Widrow-Hoff: $\overline{W} \cdot \overline{X_i}$ ì˜ ê°’ì´ <U>+1ë³´ë‹¤ ë” í´ ë•Œ(ë„ˆë¬´ ì •í™•í•  ë•Œ)ë„ ë²Œì </U>ì„ ë°›ê²Œ ëœë‹¤.

  - ì´ëŸ° ìŠµì„±ì„ ê°€ì§„ ê²ƒì€ Widrow-Hoffì˜ loss functionì´ ìœ ì¼í•˜ë‹¤.

| model | $(\overline{X_i}, y_i)$ ì˜ loss function $L_i$ |
| --- | --- |
| perceptron(smoothed surrogate) | $\max \lbrace 0, -y_i \cdot (\overline{W} \cdot \overline{X_i}) \rbrace$ |
| Widrow-Hoff | $(y_i - \overline{W} \cdot \overline{X_i})^2 = {\lbrace 1 -y_i \cdot (\overline{W} \cdot \overline{X_i}) \rbrace}^2$ |
| logistic regression | $\log(1+exp[-y_i(\overline{W} \cdot \overline{X_i})])$ |
| SVM(hinge-loss) | $\max\lbrace 0,1 - y_i \cdot (\overline{W} \cdot \overline{X_i}) \rbrace$ |
| SVM( Hinton $L_2 loss$ ) | $[\max\lbrace 0,1 - y_i \cdot (\overline{W} \cdot \overline{X_i}) \rbrace]^2$ |

---

í™•ë¥ ì„ ë‹¤ë£¨ëŠ” backpropagationì—ì„œëŠ”, $y_i \hat{y_i} < 1$ ì¸ ì ì—ì„œ ê°±ì‹ ì„ ìˆ˜í–‰í–ˆë‹¤.

$$ \overline{W} \Leftarrow \overline{W}(1- \alpha \lambda) + \alpha y_i \overline{X_i}[I(y_i \hat{y_i} < 1)] $$

- $I(\cdot) \in \lbrace 0, 1 \rbrace$ ëŠ” ì¡°ê±´ì´ ì„±ë¦½í•˜ë©´ 1ì´ ë˜ëŠ” indicator function.

ì´ ì ‘ê·¼ ë°©ì‹ì´ SVMì˜ ê¸°ë³¸ ê°±ì‹  ê³µì‹ì˜ ê°€ì¥ ë‹¨ìˆœí•œ í˜•íƒœì— í•´ë‹¹í•œë‹¤.

perceptronì€ ê°±ì‹  ìˆ˜í–‰ì„ ê²°ì •í•˜ëŠ” ì¡°ê±´ì´ $y_i\hat{y_i} < 0$ ì´ì—ˆë‹¤. ì‚¬ì‹¤ìƒ ë‘˜ì€ ê±°ì˜ ë™ì¼í•œë°, ì°¨ì´ë¥¼ ì¡°ê¸ˆ ë” ì„¤ëª…í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

- perceptronì€ <U>ì£¼ì–´ì§„ pointê°€ ì˜¤ë¶„ë¥˜ëœ ê²½ìš°ì—ë§Œ ê°±ì‹ </U>ì„ ìˆ˜í–‰í•œë‹¤.

- SVMì€ <U>ì•Œë§ê²Œ ë¶„ë¥˜ëŠ” ëìœ¼ë‚˜ í™•ì‹ ì´ ê·¸ë¦¬ ë†’ì§€ ì•Šì€ pointì—ì„œë„ ê°±ì‹ </U>ì„ ìˆ˜í–‰í•œë‹¤.

---

## 2.3 multiclass classification modelì„ ìœ„í•œ ì‹ ê²½ë§ êµ¬ì¡°

![multiclass](images/multiclass.png)

![multicalss classification model](images/multiclass_classification.png)

> ìœ„ ê·¸ë¦¼ì€ class 2ê°€ ìš°ë¦¬ê°€ ì›í•˜ëŠ” class(true)ë¼ê³  ê°€ì •í•œë‹¤. ê°€ë ¹ class 1ì€ ê°•ì•„ì§€, 2ëŠ” ê³ ì–‘ì´, 3ì€ ìë™ì°¨ì¸ ê²½ìš°ì´ë‹¤.

perceptronì˜ ê¸°ë³¸ êµ¬ì¡°ë¥¼ ì¡°ê¸ˆë§Œ ë³€ê²½í•´ë„ modelì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤.

### 2.3.1 multiclass perceptron

ê²¬ë³¸ì´ ì†í•  ìˆ˜ ìˆëŠ” classê°€ $k$ ê°œì¸ ë‹¤ë¶€ë¥˜(multiclass) ìƒí™©ì„ ê°€ì •í•˜ì.

- í›ˆë ¨ ê²¬ë³¸: $(\overline{X}_1, y_1), ... ,(\overline{X_i}, y_D)$
 
  - $d$ ì°¨ì› feature vector $\overline{X_i}$

  - class index $y \in \lbrace 1, ..., k \rbrace$

- ëª©í‘œ

  - $i$ ë²ˆì§¸ í›ˆë ¨ ê²¬ë³¸ì˜ $\overline{W_{y_i}} \cdot \overline{X_i}$ ì´, $r \neq y_i$ ì¸ $\overline{W_r} \cdot \overline{X_i}$ ë³´ë‹¤ ì»¤ì•¼ í•œë‹¤. 
  
  - ì´ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” $k$ ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ linear separator(ì„ í˜• ë¶„ë¦¬ì) $\overline{W}_1,...,\overline{W}_k$ ë¥¼ ë™ì‹œì— ë§Œì¡±í•œë‹¤ë©´ ì œëŒ€ë¡œ ëœ classificationì´ ê°€ëŠ¥í•˜ë‹¤.

ì´ëŸ° multiclass perceptronì˜ $i$ ë²ˆì§¸ í›ˆë ¨ ê²¬ë³¸ì˜ loss functionì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤.

$$ L_i = \max{}_{r:r \neq y_i} \lbrace 0, \max(\overline{W_r} \cdot \overline{X_i} - \overline{W_{y_i}} \cdot \overline{X_i}) \rbrace $$

- dot productì´ë¯€ë¡œ $\max(\overline{W_r} \cdot \overline{X_i} - \overline{W_{y_i}} \cdot \overline{X_i}) = \max \lbrace (\overline{W_r} - \overline{W_{y_i}}) \cdot \overline{X_i} \rbrace$ ë¡œ ì“¸ ìˆ˜ë„ ìˆë‹¤.

ìš”ì ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

- ì˜ˆì¸¡ê°’ $\hat{y_i}$ ê°€ ì˜¬ë°”ë¥¸ classì— í•´ë‹¹í•˜ë©´( $y_i = \hat{y_i}$ ), ê°±ì‹ ì€ ì¼ì–´ë‚˜ì§€ ì•ŠëŠ”ë‹¤.

- ì˜ˆì¸¡ê°’ $\hat{y_i}$ ê°€ ë‹¤ë¥¸ classì— í•´ë‹¹í•˜ë©´( $y_i \neq \hat{y_i}$ ), ë‘ ê°€ì§€ ê°±ì‹ ì„ ì§„í–‰í•œë‹¤.( $\alpha$ ëŠ” learning rate )

  - correct-class vectorë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ê°±ì‹ : $\overline{W_{y_i}} \Leftarrow \overline{W_{y_i}} + \alpha\overline{X_i}$

  - wrong-class vectorë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ê°±ì‹ : $\overline{W_{\hat{y_i}}} \Leftarrow \overline{W_{\hat{y_i}}} - \alpha\overline{X_i}$

ë”°ë¼ì„œ dataë§ˆë‹¤ í•­ìƒ ëª¨ë“  nodeì˜ weightê°€ ê°±ì‹ ë˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, <U>í•´ë‹¹í•˜ëŠ” ë‘ ê°œë§Œ ê°±ì‹ </U>ëœë‹¤.

---

### 2.3.2 Weston-Watkins SVM

Weston-Watkins(ì›¨ìŠ¤í„´-ì™“í‚¨ìŠ¤) SVMì€ ìœ„ multiclass perceptronì—ì„œ ë‹¤ìŒ ë‘ ê°€ì§€ë¥¼ ìˆ˜ì •í•œ í˜•íƒœë‹¤.

1. multiclass perceptronì´ trueì™€ wrongì— í•´ë‹¹ë˜ëŠ” ë‘ weightsë§Œ ê°±ì‹ í•˜ì§€ë§Œ, Weston-Watkins SVMì€ true classë³´ë‹¤ ê²¬ë³¸ì— ë” ì í•©í•˜ë‹¤ê³  ì˜ˆì¸¡ëœ **ì„ì˜ì˜** class weightsë“¤ì„ ê°±ì‹ í•œë‹¤.

2. Weston-Watkins SVMì€ ì˜¤ë¶„ë¥˜ëœ classì˜ weightsì„ ê°±ì‹ í•˜ë©´ì„œ, ë˜í•œ true classì— ë„ˆë¬´ ê°€ê¹Œìš´ classì˜ weightë„ ê°±ì‹ í•œë‹¤. ì´ëŠ” margin ê°œë…ì„ ì´ìš©í•œë‹¤.

Weston-Watkins SVMì˜ $i$ ë²ˆì§¸ í›ˆë ¨ ê²¬ë³¸ì˜ loss functionì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$ L_i = \sum_{r:r \neq y_i} \max(0, \overline{W_r} \cdot \overline{X_i} - \overline{W_{y_i}} \cdot \overline{X_i} + 1) $$

1ì„ ë”í•´ì£¼ë©´ì„œ $\overline{W_{y_i}}$ ì™€ <U>marginì´ 1ì´ ì•ˆ ë˜ê²Œ true classë¡œ ê°€ê¹ê²Œ ë¶„ë¥˜í•˜ëŠ” $\overline{W_r}$ ë„ ê³ ë ¤</U>ë¥¼ í•˜ê²Œ ë°”ë€Œì—ˆë‹¤. ë˜í•œ í•©ì„ ê³„ì‚°í•˜ëŠ” í˜•íƒœë¼ì„œ <U>true classë³´ë‹¤ ê²¬ë³¸ì— ë” ì í•©í•˜ê²Œ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë“  weightsì„ ê³ ë ¤</U>í•œë‹¤.

ê°±ì‹ ì„ ê³ ë ¤í•´ì•¼ í•˜ëŠ” ë¶€ë¶„ì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ë¦¬í•  ìˆ˜ ìˆë‹¤.

- ì˜ˆì¸¡ê°’ $\hat{y_i}$ ê°€ ì˜¬ë°”ë¥¸ classì— í•´ë‹¹í•˜ê³ ( $y_i = \hat{y_i}$ ) $\overline{W_{y_i}}$ ê°€ ì´ë¥¼ ê°€ì¥ ì˜ ë¶„ë¥˜í•˜ë©°, ê·¸ ë‹¤ìŒìœ¼ë¡œ $y_i$ ì— ì í•©í•˜ë‹¤ê³  ì˜ˆì¸¡ëœ classê°€ ì¶©ë¶„í•œ marginì„ ê°€ì§€ê³  ìˆë‹¤ë©´ ê°±ì‹ ì€ ì¼ì–´ë‚˜ì§€ ì•ŠëŠ”ë‹¤.

- ìœ„ ì¡°ê±´ì— í•´ë‹¹í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ ë‹¤ìŒ ê°±ì‹ ì„ ì§„í–‰í•œë‹¤. regularizationì„ í¬í•¨í•œ ê°±ì‹ ì´ë‹¤.( $\alpha$ ëŠ” learning rate )

  > $\overline{W_r}$ ì´ loss functionì— ì–‘ì˜ ê°’ì„ ê¸°ì—¬í•˜ë©´ 1ì„ ë°˜í™˜í•˜ëŠ” 0/1 indicator functionì„ $\delta(r, \overline{X_i})$ ë¼ê³  í‘œê¸°í•˜ì.

  - correct-class vector ê°±ì‹ : $\overline{W_{y_i}} \Leftarrow \overline{W_{y_i}}(1 - \alpha \lambda) + \alpha\overline{X_i}[\sum_{j \neq r}{\delta(j,\overline{X_i})}]$

  - wrong-class vector ê°±ì‹ : $\overline{W_{\hat{y_i}}} \Leftarrow \overline{W_{\hat{y_i}}}(1 - \alpha \lambda) - \alpha\overline{X_i}[{\delta(r,\overline{X_i})}]$

ì—¬ê¸°ì„œ $\overline{W_r}$ ì´ loss functionì— ì–‘ì˜ ê°’ì„ ê¸°ì—¬í•œë‹¤ëŠ” ë§ì€ ì¦‰, true classì˜ $\overline{W_{y_i}}$ ë³´ë‹¤ ë” ì í•©í•˜ë‹¤ê³  ì˜ˆì¸¡í•˜ëŠ” $\overline{W_r}$ ì´ ìˆë‹¤ëŠ” ëœ»ì´ë‹¤.

correct-class vectorëŠ” ë” ì í•©í•˜ë‹¤ê³  ì˜ˆì¸¡í•˜ëŠ” $\overline{W_r}$ ì´ ì ì„ìˆ˜ë¡ ë” í¬ê²Œ ê°±ì‹ í•˜ê²Œ ë˜ê³ , wrong-class vectorëŠ” ë” ì í•©í•˜ê²Œ ì˜ˆì¸¡í•˜ëŠ” $\overline{W_r}$ ì˜ ì¡´ì¬ ì—¬ë¶€ì— ë”°ë¼ ê°±ì‹ í•˜ê²Œ ëœë‹¤.

> SVMì´ ì œëŒ€ë¡œ ì‘ë™í•˜ë ¤ë©´ ì´ëŸ° regularizationì´ ë°˜ë“œì‹œ í•„ìš”í•˜ë¯€ë¡œ, regularizationì„ ê³ ë ¤í•œ ì‹ìœ¼ë¡œ ê¸°ì–µí•˜ì.

---

### 2.3.3 multinomial logistic regression(softmax regression)

multinomial logistic regression(ë‹¤í•­ ë¡œì§€ìŠ¤í‹± íšŒê·€) = softmax regressionë„ ìœ„ Weston-Watkins SVM ì‚¬ë¡€ì²˜ëŸ¼ logistic regressionì„ multinomialí•œ ë°©ì‹ìœ¼ë¡œ ì¼ë°˜í™”í•œ ê²ƒì´ë‹¤.

ê·¸ëŸ¬ë‚˜ ì´ì „ perceptronê³¼ SVMê³¼ ë‹¤ë¥¸ ì ì€, ê°€ëŠ¥ë„ë¥¼ posteriori probability(ì‚¬í›„í™•ë¥ ) $P(r|\overline{X_i})$ ë¡œ ê³ ë ¤í•œë‹¤ëŠ” ì ì´ë‹¤. 

> ì‚¬í›„í™•ë¥  $P(A|B)$ ëŠ” ê´€ì¸¡ Bë¥¼ ë³´ê³  ì›ì¸ì´ Aë¼ê³  ìƒê°ë˜ëŠ” í™•ë¥ ì´ë‹¤.

ì´ëŸ° ì‚¬í›„í™•ë¥ ì„ softmaxë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶”ì •í•  ìˆ˜ ìˆë‹¤. ì´ëŸ° ë°©ì‹ìœ¼ë¡œ classì— í•´ë‹¹í•˜ëŠ” membership(ì†Œì†ë„)ë¥¼ í™•ë¥ ê°’ìœ¼ë¡œ ì˜ˆì¸¡í•œë‹¤.

$$ P(r|\overline{X_i}) = {{\exp(\overline{W_r} \cdot \overline{X_i})} \over {\sum_{j=1}^{k}\exp(\overline{W}_j \cdot \overline{X_i})}} $$

ì´ softmax regressionì˜ $i$ ë²ˆì§¸ í›ˆë ¨ ê²¬ë³¸ì˜ loss functionì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$ L_i = - \log[P(y_i|\overline{X_i})] $$

$$ = - \overline{W_{y_i}} \cdot \overline{X_i} + \log[\sum_{j=1}^{k} \exp (\overline{W}_j \cdot \overline{X_i})] $$

- softmax í™œì„±í™” ì „ ê°’ì„ $v_r = \overline{W_r} \cdot \overline{X_i}$ ë¡œ í‘œí˜„í•˜ë©´ ë” ê°„ë‹¨íˆ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.

$$ = - v_{y_i} + \log[\sum_{j=1}^{k} \exp (v_j)] $$

softmax regression ì—­ì‹œ ì˜ˆì¸¡ê°’ $\hat{y_i}$ ê°€ ì˜¬ë°”ë¥¸ classì— í•´ë‹¹í•˜ëŠ”ì§€ì— ë”°ë¼ ê°±ì‹ ì„ ë‹¤ë¥´ê²Œ ì ìš©í•œë‹¤.

> backpropagationì„ ìœ„í•´ chain ruleì„ ì´ìš©í•œë‹¤. ${{\partial L_i} \over {\partial \overline{W_r}}} = {{\partial L_i} \over {\partial v_r}} \cdot {{\partial v_r} \over {\partial \overline{W_r}}} = {{\partial L_i} \over {\partial v_r}} \cdot \overline{X_i}$

- ì˜ˆì¸¡ê°’ $\hat{y_i}$ ê°€ ì˜¬ë°”ë¥¸ classì— í•´ë‹¹í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ê°±ì‹ í•œë‹¤.

$$ \overline{W_{y_i}} \Leftarrow \overline{W_{y_i}}(1 - \alpha \lambda) + \alpha\overline{X_i} \cdot (1 - P(y_i|\overline{X_i})) $$

- ì˜ˆì¸¡ê°’ $\hat{y_i}$ ê°€ ë‹¤ë¥¸ classì— í•´ë‹¹í•˜ë©´( $y_i \neq \hat{y_i}$ ), ë‹¤ìŒê³¼ ê°™ì´ ê°±ì‹ í•œë‹¤.

$$ \overline{W_{\hat{y_i}}} \Leftarrow \overline{W_{\hat{y_i}}}(1 - \alpha \lambda) - \alpha\overline{X_i} \cdot P(\hat{y_i}|\overline{X_i}) $$

ì¦‰ ì˜ˆì¸¡ê°’ $\hat{y_i}$ ê°€ í° í™•ë¥ ë¡œ ì˜¬ë°”ë¥¸ classë¡œ ì˜ˆì¸¡í• ìˆ˜ë¡ $\overline{W_{y_i}}$ ëŠ” ì¡°ê¸ˆë§Œ ì»¤ì§€ë„ë¡ ê°±ì‹ ëœë‹¤. ë°˜ë©´ ì˜ˆì¸¡ê°’ $\hat{y_i}$ ê°€ ë‹¤ë¥¸ classì— í° í™•ë¥ ë¡œ í•´ë‹¹í•œë‹¤ê³  ì˜ˆì¸¡í• ìˆ˜ë¡ $\overline{W_{\hat{y_i}}}$ ëŠ” í¬ê²Œ ì‘ì•„ì§€ë„ë¡ ê°±ì‹ ëœë‹¤.

softmax regressionì€ multiclass perceptronì´ë‚˜ Weston-Watkins SVMê³¼ ë‹¬ë¦¬, <U>ê° í›ˆë ¨ ê²¬ë³¸ë§ˆë‹¤ $k$ ê°œì˜ seperator $\overline{W}_1,...,\overline{W}_k$ ë¥¼ ëª¨ë‘ ê°±ì‹ </U>í•œë‹¤.

---

### 2.3.4 hierarchical softmax

ë§Œì•½ classê°€ êµ‰ì¥íˆ ë§ë‹¤ë©´, softmax regressionì€ ë§¤ë²ˆ seperatorë¥¼ ê°±ì‹ í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— í•™ìŠµì´ ë§¤ìš° ëŠë ¤ì§ˆ ê²ƒì´ë‹¤. 

> ì£¼ë¡œ text miningê³¼ ê°™ì´ target wordë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê³¼ì œê°€ ì´ëŸ° ê²½ìš°ì— í•´ë‹¹í•œë‹¤.

ì´ëŸ° ë¬¸ì œë¥¼ hierarchical softmax(ìœ„ê³„ì  ì†Œí”„íŠ¸ë§¥ìŠ¤)ë¥¼ ì´ìš©í•˜ì—¬ í•™ìŠµ ì†ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆë‹¤. ì´ ë°©ë²•ì˜ í•µì‹¬ì€ classë“¤ì„ hierarchicalí•˜ê²Œ ë¬¶ì–´ì„œ binary treeì™€ ë¹„ìŠ·í•œ ê³„í†µ êµ¬ì¡°ë¥¼ ë§Œë“œëŠ” ê²ƒì— ìˆë‹¤.

![binary tree for hierarchical softmax](images/binary_tree_hierarchical_softmax.jpeg)

> ë¶„ëª¨ë¥¼ ë‹¤ ë”í•˜ì§€ ì•Šê³  í™•ë¥ ì„ êµ¬í•´ë³´ë ¤ëŠ” ì•„ì´ë””ì–´ë¼ëŠ” ê´€ì ì—ì„œ ë³´ì.

ê°€ë ¹ ìœ„ ê·¸ë¦¼ì²˜ëŸ¼ $w_4$ ë¼ëŠ” ë‹¨ì–´ì˜ ì£¼ë³€ë¶€ ë‹¨ì–´ê°€ $w_2$ ë¼ëŠ” ë‹¨ì–´ì´ê³ , ì´ ë‘˜ì˜ ê´€ê³„ ì •ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í™•ë¥ ê°’ì„ ë§Œë“¤ê³  ì‹¶ë‹¤ê³  í•˜ì. 

tree êµ¬ì¡°ì˜ root node(ë¿Œë¦¬ ë…¸ë“œ)ì—ì„œ terminal node(ë§ë‹¨ ë…¸ë“œ, leaf node)ê¹Œì§€ $\log_{2}(k)$ íšŒ ì´ì§„ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•´ì„œ $k$ ì¤‘ ë¶„ë¥˜ ê²°ê³¼ë¥¼ ì–»ëŠ”ë‹¤.(ë§ë‹¨ì— ê° ë‹¨ì–´ë“¤ì´ ìœ„ì¹˜í•˜ê²Œ ëœë‹¤.)

ê·¸ë¦¬ê³  terminal nodeê¹Œì§€ ê°€ë©´ì„œ ë§Œë‚˜ëŠ” nodeì™€ ì—°ì‚°ì„ ìˆ˜í–‰í•˜ë©´ì„œ, ìµœì¢…ì ìœ¼ë¡œ ë„ì¶œí•˜ëŠ” í™•ë¥ ê°’ì„ ì‚¬ìš©í•˜ê²Œ ëœë‹¤.

ê·¸ë ‡ë‹¤ë©´ classë¥¼ ì–´ë–»ê²Œ hierarchicalí•˜ê²Œ ë¬¶ì„ ìˆ˜ ìˆì„ê¹Œ? í•œ ê°€ì§€ ì ‘ê·¼ ë°©ì‹ìœ¼ë¡œ ê·¸ëƒ¥ randomí•˜ê²Œ tree êµ¬ì¡°ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì´ë‹¤. classê°€ ë¹„ìŠ·í•œ classê¹Œì§€ ë¬¶ì˜€ë‹¤ë©´ ì„±ëŠ¥ì´ í–¥ìƒë  ê²ƒì´ë‹¤. ë˜í•œ Huffman encoding(í—ˆí”„ë¨¼ ë¶€í˜¸í™”)ë¥¼ ì´ìš©í•´ì„œ binary treeë¥¼ ë§Œë“œëŠ” ë°©ë²•ë„ ìˆë‹¤.

> ì˜ˆë¥¼ ë“¤ì–´ target word ì˜ˆì¸¡ì€ WordNet ê³„í†µêµ¬ì¡°ì— ë”°ë¼ classë¥¼ ë¬¶ìœ¼ë©´ ë„ì›€ì´ ëœë‹¤.(ë‹¤ë§Œ binary tree êµ¬ì¡°ê°€ ì•„ë‹ˆë¯€ë¡œ ì¶”ê°€ì ì¸ ì¬ì¡°ì§í™”ëŠ” í•„ìš”í•˜ë‹¤.)

---