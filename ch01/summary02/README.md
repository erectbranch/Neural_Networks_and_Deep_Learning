## 1.4 ì‹ ê²½ë§ í›ˆë ¨ì˜ ì‹¤ì§ˆì ì¸ ë¬¸ì œì ë“¤

## 1.4.1 overfitting(ê³¼ëŒ€ì í•©)

ì‹ ê²½ë§ í›ˆë ¨ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ë¬¸ì œì ìœ¼ë¡œëŠ” overfitting(ê³¼ëŒ€ì í•©)ì´ ê¼½íŒë‹¤. í•œ modelì„ train data setì„ ì´ìš©í•´ì„œ í›ˆë ¨ì‹œí‚¨ ë’¤, ëª¨í˜•ì´ labelì„ ì™„ë²½í•˜ê²Œ ì˜ˆì¸¡í–ˆë‹¤ê³  í•´ë„, ë¯¸ì§€ì˜ test dataì— ëŒ€í•´ ì¢‹ì€ ì˜ˆì¸¡ ì„±ê³¼ë¥¼ ë‚´ë¦¬ë¼ëŠ” ë³´ì¥ì´ ì—†ë‹¤ëŠ” ê²ƒì´ë‹¤.

> train dataì— ë„ˆë¬´ ì™„ë²½í•˜ê²Œ ë§ì¶°ì§€ë©´ì„œ training errorê°€ 0ì´ ë˜ë©´, ë°˜ëŒ€ë¡œ ì‹¤ì œ ë°ì´í„°ì—ëŠ” ì œëŒ€ë¡œ ëœ ì˜ˆì¸¡ì„ í•  ìˆ˜ ì—†ì„ ê²ƒì´ë‹¤. 

> í•™ìŠµì— ì‚¬ìš©ë˜ì§€ ì•Šì€ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ í‰ê°€í•˜ëŠ” ê³¼ì •ì„ testë¥¼ í•œë‹¤ê³  í•˜ê³ , ì´ë¥¼ **Generalization**(ì¼ë°˜í™”)ë¼ê³  í‘œí˜„í•œë‹¤. train dataì™€ test dataëŠ” ì„œë¡œ êµì ì´ ë˜ëŠ” ë°ì´í„°ë“¤ì´ ì—†ê¸° ë•Œë¬¸ì—, testë¥¼ Generalizationì´ë¼ê³  í•˜ê³  test errorë¥¼ generalization errorë¼ê³  í‘œí˜„í•˜ëŠ” ê²ƒì´ë‹¤.

train dataë¥¼ ëŠ˜ë¦¬ë©´ modelì˜ ì¼ë°˜í™” ì„±ëŠ¥ì´ ê°œì„ ë˜ëŠ” ë°˜ë©´, modelì˜ ë³µì¡ë„ë¥¼ ëŠ˜ë¦¬ë©´ ì¼ë°˜í™” ì„±ëŠ¥ì´ ì¤„ì–´ë“ ë‹¤. ê·¸ë ‡ë‹¤ê³  ëª¨í˜•ì´ ë„ˆë¬´ ë‹¨ìˆœí•˜ë©´ ì œëŒ€ë¡œ featureë¥¼ íŒŒì•…í•˜ì§€ ëª»í•  ìœ„í—˜ì´ ìˆë‹¤.

> ì¼ë°˜ì ì¸ ë²•ì¹™ìœ¼ë¡œ ì „ì²´ í›ˆë ¨ì  ê°œìˆ˜ê°€, ì‹ ê²½ë§ ë§¤ê°œë³€ìˆ˜ ê°œìˆ˜ì˜ ì ì–´ë„ 2~3ë°°ëŠ” ë˜ì–´ì•¼ í•œë‹¤.

> ì¼ë°˜ì ìœ¼ë¡œ, ë§¤ê°œë³€ìˆ˜ê°€ ë§ì€ modelì„ ê°€ë¦¬ì¼œ '**capacity**(ìˆ˜ìš©ë ¥)ì´ ë†’ë‹¤'ë¼ê³  í‘œí˜„í•œë‹¤. ì¦‰, high capacity modelê°€ ì˜ ì¼ë°˜í™”ë˜ê¸° ìœ„í•´ì„œëŠ” ë§ì€ í›ˆë ¨ ìë£Œê°€ í•„ìš”í•˜ë‹¤.

ë³´í†µ ê¸°ê³„í•™ìŠµì—ì„œ overfittingì„ biasì™€ varianceì˜ trade-off ê´€ì ìœ¼ë¡œ ë³¼ ë•Œê°€ ë§ë‹¤. ëª¨í˜•ì˜ ë³µì¡ë„ë¥¼ ê²°ì •í•  ë•ŒëŠ” ìµœì ì˜ ì§€ì ì„ ì„¸ì‹¬í•˜ê²Œ ì„ íƒí•  í•„ìš”ê°€ ìˆë‹¤.

> ì‹ ê²½ë§ì´ ê±°ì˜ ëª¨ë“  ì¢…ë¥˜ì˜ í•¨ìˆ˜ë¥¼ í‰ë‚´ë‚¼ ìˆ˜ ìˆì„ ì •ë„ë¡œ ê°•ë ¥í–ˆì§€ë§Œ, ê·¸ë™ì•ˆ ì¸ê¸°ê°€ ì—†ì—ˆë˜ ì´ìœ ê°€ ë°”ë¡œ ì´ëŸ° ë‹¨ì  ë•Œë¬¸ì´ë‹¤. ì§€ê¸ˆì€ ê°€ìš©í•  ìˆ˜ ìˆëŠ” dataê°€ ë§ì•„ì§€ë©´ì„œ ê¸°ì¡´ ê¸°ê³„ í•™ìŠµì— ë¹„í•´ ì‹ ê²½ë§ì˜ ì¥ì ì´ ë§¤ìš° ë‘ë“œëŸ¬ì§„ ê²ƒì´ë‹¤.

ì´ì œ overfittingì˜ ì˜í–¥ì„ ì™„í™”í•˜ëŠ” ëª‡ ê°€ì§€ ì„¤ê³„ ë°©ë²•ì„ ì†Œê°œí•  ê²ƒì´ë‹¤.

---

## 1.4.1.1 regularization(ì •ì¹™í™”)

> [Regularization (Weight Decay)](https://deepapple.tistory.com/6)

ë§¤ê°œë³€ìˆ˜ê°€ ë§ì„ìˆ˜ë¡ overfittingì´ ì¼ì–´ë‚˜ë¯€ë¡œ, ì´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ëª¨í˜•ì—ì„œ (0ì„ ì œì™¸í•œ) <U>ë§¤ê°œë³€ìˆ˜ì˜ ê°œìˆ˜ë¥¼ ì¤„ì´ëŠ” ë°©í–¥</U>ìœ¼ë¡œ ì ‘ê·¼í•  ìˆ˜ ìˆë‹¤.

ë˜í•œ <U>ë§¤ê°œë³€ìˆ˜ì˜ ì ˆëŒ“ê°’ì˜ í¬ê¸°ë¥¼ ì¤„ì—¬ë„ overfittingì´ ì™„í™”</U>ë˜ëŠ” ê²½í–¥ì´ ìˆëŠ”ë°, ê·¸ë ‡ë‹¤ê³  ë§¤ê°œë³€ìˆ˜ ê°’ ìì²´ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ì œí•œí•˜ëŠ” ê²ƒì€ ì–´ë µë‹¤. ê·¸ë˜ì„œ loss functionì— penalty í•­ $ \lambda || \overline{W} ||^p $ ë¥¼ ë„ì…í•˜ëŠ” ì¢€ ë” ì˜¨ê±´í•œ ì ‘ê·¼ ë°©ì‹ì´ ì“°ì¸ë‹¤. 

> ë°ì´í„°ê°€ ë‹¨ìˆœí•˜ê³  ëª¨ë¸ì´ ë³µì¡í•˜ë©´, trainingì„ í•˜ë©´ì„œ ì‘ì€ ê°’ì´ì—ˆë˜ weightë“¤ì˜ ê°’ì´ ì ì  ì¦ê°€í•œë‹¤. weightê°€ ì»¤ì§ˆìˆ˜ë¡ train dataê°€ ëª¨ë¸ì— ì£¼ëŠ” ì˜í–¥ë ¥ì´ ì»¤ì§€ê³ , ê²°êµ­ ëª¨ë¸ì´ train dataì— ë”± ë§ì¶°ì§€ê²Œ ëœë‹¤. ì´ê²ƒì„ local noiseì˜ ì˜í–¥ì„ í¬ê²Œ ë°›ì•„ì„œ, outlierë“¤ì— ëª¨ë¸ì´ ë§ì¶°ì§€ëŠ” í˜„ìƒì´ë¼ê³  í‘œí˜„í•œë‹¤.

ì´ë•Œ $p$ ëŠ” í”íˆ 2ë¡œ ì„¤ì •í•˜ëŠ”ë°, ì´ëŠ” **L2 regularization**(Tikhonov regularization, í‹°í˜¸ë…¸í”„ ì •ì¹™í™”)ì— í•´ë‹¹í•œë‹¤. L2 regularizationì—ì„œëŠ” ê° ë§¤ê°œë³€ìˆ˜(ì— ì •ì¹™í™” ë§¤ê°œë³€ìˆ˜ $\lambda >0 $ ì„ ê³±í•œ ê°’)ì„ ì œê³±í•œ ê²°ê³¼ë¥¼ loss functionì— ë”í•´ì¤€ë‹¤.

---

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ”’ ì •ì˜: L1, L2 Norm/Regularization&nbsp;&nbsp;&nbsp;</span>

> [L1 Regularization, L2 Regularization](https://light-tree.tistory.com/125)

1. **Norm**

**Norm**ì€ ë²¡í„°ì˜ í¬ê¸°(í˜¹ì€ ê¸¸ì´)ë¥¼ ì¸¡ì •í•˜ëŠ” ë°©ë²•(í˜¹ì€ í•¨ìˆ˜)ë¥¼ ëœ»í•œë‹¤. 

$$ ||x||_{p} := \left( \sum_{i=1}^{n}{|x_i|}^p \right)^{1/p} $$

- L1 Norm

$$ d_{1}(p,q) = {||p-q||}_{1} = \sum_{i=1}^{n}{|p_i - q_i|} $$

- $ p = (p_1, p_2, ..., p_n), \quad q = (q_1, q_2, ..., q_n) $

ì‰½ê²Œ ë§í•˜ë©´ ë‘ ë²¡í„° $ p, q $ ì˜ ê° ì›ì†Œë“¤ ê°„ ì°¨ì´ì˜ ì ˆëŒ“ê°’ë“¤ì„ êµ¬í•œ ë’¤ ëª¨ë‘ ë”í•œ ê°’ì´ë‹¤. 

- L2 Norm

$$ {||x||}_2 := \sqrt{{x_{1}}^{2} + ... + {x_{n}}^2} $$

- $ p = (p_1, p_2, ..., p_n), \quad q = (0, 0, ..., 0) $

- ê³µì‹ì„ ê°„í¸í•˜ê²Œ ì‘ì„±í•˜ê¸° ìœ„í•´ që¥¼ ì›ì ìœ¼ë¡œ ë’€ì„ ë¿, ì‹¤ì œë¡œ ê°’ì´ ìˆë‹¤ë©´ ì°¨ì´ë¥¼ ëŒ€ì…í•˜ë©´ ëœë‹¤.

<br/>

2. **Regularization**

- L1 Regularization

ê¸°ì¡´ì˜ cost function(loss functionì˜ í‰ê· )ì— $ \lambda |w| $ ë¥¼ ë”í•œ ê°’ì„ cost functionìœ¼ë¡œ ì“°ê²Œ ëœë‹¤.

ì˜ˆë¥¼ ë“¤ì–´ ë‹¤ìŒê³¼ ê°™ì€ ìˆ˜ì‹ì´ ìˆë‹¤ê³  ê°€ì •í•˜ì.

$$ H(X) = w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4 $$

ì—¬ê¸°ì— L1 Regularizationì„ ì ìš©í–ˆì„ ë•Œ $ w_3 $ ì˜ ê°’ì´ 0ì´ ë˜ì—ˆë‹¤ë©´, ì´ ë§ì€ $ x_3 $ ì˜ featureì´ ì‚¬ì‹¤ modelì˜ ê²°ê³¼ì— ë³„ ì˜í–¥ì„ ì£¼ì§€ ëª»í•œë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤.

> ëŒ€ë¶€ë¶„ì˜ êµ¬í˜„ì—ì„œ L2 Regularizationì„ ì„ í˜¸í•˜ì§€ë§Œ, L1 Regularizationë„ ë‚˜ë¦„ì˜ ìš©ë„ê°€ ìˆë‹¤. inputì—ì„œ ë‚˜ê°€ëŠ” edgeì˜ $ w_i $ ì˜ ê°’ì´ 0ì´ë©´ í•´ë‹¹ inputì€ ìµœì¢… outputì— ì•„ë¬´ëŸ° ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•Šê²Œ ëœë‹¤. ì¦‰, ê·¸ëŸ° inputì´ ìƒëµë˜ë©´ì„œ(dropped) ì¼ì¢…ì˜ feature ì„ íƒê¸°ë¡œ ì‘ìš©í•œë‹¤. ë”°ë¼ì„œ L1 Regularizationì„ ì ìš©í•˜ë©´ì„œ featureì˜ ì˜í–¥ì„ íŒŒì•…í•´ ë³¼ ìˆ˜ ìˆë‹¤.

- L2 Regularization

ë²¡í„°ì˜ í¬ê¸°ë¥¼ ì˜ë¯¸í•˜ëŠ” Normì„ ì´ìš©í•´ì„œ, ê°€ì¤‘ì¹˜ ë²¡í„° í¬ê¸°ë§Œí¼ì˜ penaltyë¥¼ ë¶€ì—¬í•˜ëŠ” ê°œë…ì´ ë°”ë¡œ regularizationì´ë‹¤.

> ì´ëŸ° penalty ë¶€ì—¬ë¥¼ ê°±ì‹  ë„ì¤‘ ì¼ì¢…ì˜ 'weight decay(ê°€ì¤‘ì¹˜ ê°ì‡ )ë¥¼ ì ìš©'í•˜ëŠ” ê²ƒìœ¼ë¡œ ë´ë„ ë¬´ë°©í•˜ë‹¤.

weight decayëŠ” ê¸°ì¡´ì˜ cost functionì— í•­ì„ ì¶”ê°€í•´ì„œ í° ê°’ì„ ê°€ì§€ëŠ” weightì— penaltyë¥¼ ë¶€ì—¬í•˜ëŠ” ê²ƒì´ë‹¤. ì´ë•Œ L2 norm í•­ì´ weight íŒ¨ëŸ¬ë¯¸í„°ì˜ í¬ê¸°ë¥¼ ë‚˜íƒ€ë‚´ê¸° ë•Œë¬¸ì—, í˜„ì¬ weightì˜ í¬ê¸°ì— ë¹„ë¡€í•´ì„œ ë”í•´ì£¼ëŠ” ê°’ì´ ì»¤ì§„ë‹¤.

$$ E(w) = E_0(w) + {1 \over 2}\lambda\sum_{i}{w_{i}^2} $$

- $E_{0}(w)$ : ê¸°ì¡´ cost function

> ì•ì— 1/2ê°€ ë¶™ëŠ” ê²ƒì€ ë¯¸ë¶„ì˜ í¸ì˜ì„±ì„ ìœ„í•´ì„œì´ë©°, ì•„ì˜ˆ ì•ˆ ì“°ê±°ë‚˜ 1/Nìœ¼ë¡œ í‘œê¸°í•˜ëŠ” ê²½ìš°ë„ ë§ë‹¤. 

wë¥¼ í–‰ë ¬ë¡œ ë‹¤ì‹œ ì“°ë©´ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.

$$ E(W) = MSE_{train} + {1 \over 2}\lambda W^{\mathsf{T}}W $$

ê·¸ë ‡ë‹¤ë©´ ì™œ ì´ ê°’ì„ ë”í•´ì£¼ëŠ” ê²ƒì´ penaltyë¡œ ì‘ìš©í• ê¹Œ? ê·¸ ì´ìœ ëŠ” gradient descentë¥¼ ìƒê°í•´ ë³´ë©´ ì•Œ ìˆ˜ ìˆë‹¤.

gradient descentëŠ” errorì˜ ìµœì†Ÿê°’ì„ ì°¾ì•„ê°€ëŠ” ê³¼ì •ì´ë‹¤. ê·¸ëŸ°ë° error ê°’ì— ì¼ì • ë¹„ìœ¨ 'ê°€ì¤‘ì¹˜ í¬ê¸°'ì— í•´ë‹¹í•˜ëŠ” ê°’ì„ ë”í•œë‹¤ë©´, ê°€ì¤‘ì¹˜ì˜ ì ˆëŒ€ì ì¸ í¬ê¸°ê°€ í´ìˆ˜ë¡ errorê°€ ì»¤ì§€ê³  ë°˜ì˜ì€ ê²°ê³¼ì ìœ¼ë¡œ ëœ ì´ë£¨ì–´ì§€ê²Œ ëœë‹¤.

---

ì•„ë˜ëŠ” ì •ì¹™í™”ë¥¼ ì ìš©í•œ ê°±ì‹  ê³µì‹ì´ë‹¤.

$$ \overline{W} \Leftarrow \overline{W}(1-a\lambda) + \alpha \sum_{\overline{X} \in S}{E(\overline{X})\overline{X}} $$

- $E(\overline{X})$ : ì˜¤ì°¨ $ (y - \hat{y})$ (ëª¨í˜•ë§ˆë‹¤ ê³ ìœ ì˜ ì˜¤ì°¨í•¨ìˆ˜ê°€ ë“¤ì–´ê°„ë‹¤.)

- $S$ : mini batch(ë¯¸ë‹ˆë°°ì¹˜)

- $ \alpha $ : learning rateë¥¼ ì¡°ì •í•˜ëŠ” ë§¤ê°œë³€ìˆ˜

- $ \lambda $ : ì •ì¹™í™”ì˜ ì •ë„ë¥¼ ì¡°ì ˆí•˜ëŠ” ì •ì¹™í™” ë§¤ê°œë³€ìˆ˜. 0ì´ë©´ ì›ë˜ì˜ loss functionì´ ë˜ê³ , 0ë³´ë‹¤ í¬ë©´ ê¸°ìš¸ê¸° ë³€í™”ê°€ ë„ˆë¬´ ì»¤ì§€ì§€ ì•Šë„ë¡ ê°ì‡ í•œë‹¤.(ëŒ€ì²´ë¡œ 0.01 ~0.00001)

> ìƒë¬¼í•™ì ìœ¼ë¡œ ë¹„ìœ í•˜ë©´ ë‡Œì—ì„œ ëœ ì¤‘ìš”í•œ, ì¦‰ noise íŒ¨í„´ì´ ì œê±°ë˜ë©´ì„œ ë°œìƒí•˜ëŠ” 'ì ì§„ì  ë§ê°'ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤.

![regularization ì ìš© ì „í›„ ë¹„êµ](images/regularization_before_after.png)

ì•„ë˜ ê·¸ë¦¼ì€ í•˜ì´í¼íŒ¨ëŸ¬ë¯¸í„° $ \lambda $ ê°’ì— ë”°ë¥¸ multinomial regression ê²°ê³¼ì´ë‹¤.

![lambda ê°’ì— ë”°ë¥¸ multinomial regression](images/regularization_lambda.png)

### ì •ì¹™í™”ì˜ ê²°ì 

[Regularization in Neural Network](https://cedar.buffalo.edu/~srihari/CSE574/Chap5/Chap5.5-Regularization.pdf)

1. **Invariance to Transformation**

í•˜ì§€ë§Œ weight decayë¥¼ ì ìš©í•˜ê¸° ì „ì— ëª‡ëª‡ <U>scaling ê³¼ì •ì„ ì ìš©í–ˆë‹¤ë©´</U> ì§€ê¸ˆì˜ ë°©ë²•ì„ ì ìš©í•  ìˆ˜ ì—†ê²Œ ëœë‹¤. ì˜ˆì‹œë¡œ 2 layerë¡œ êµ¬ì„±ëœ ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ ì— linear transformì„ ì ìš©í•˜ë©´ ì–´ë–»ê²Œ weight decayë¥¼ ì“¸ ìˆ˜ ì—†ê²Œ ë˜ëŠ”ì§€ ë³´ì.

2 layerë¡œ êµ¬ì„±ëœ ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ ì´ ìˆë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ì€ í˜•íƒœì¼ ê²ƒì´ë‹¤.

- $ {x_1, x_2, ..., x_i} $ : input

- $ {y_1, y_2, ..., y_k} $ : output

- ì²« hidden layerì˜ jë²ˆì§¸ unitì˜ output $ z_j $ ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.(activation functionê¹Œì§€ ì ìš©í•œë‹¤.) ( $ w_{j0}$ ì€ bias )

$$ z_j = h \left( \sum_{i}{w_{ji}x_i + w_{j0}} \right) $$

- output unitì˜ activationì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$ y_k \ \sum_{j}{w_{kj}z_j + w_{k0}} $$

1. inputì— linear transformì„ ì ìš©

ê·¸ëŸ°ë° input dataì— linear transformì„ ì ìš©í–ˆë‹¤ë©´ inputì€ ë‹¤ìŒê³¼ ê°™ì€ í˜•íƒœê°€ ëœë‹¤.

$$ x_i = \tilde{x}_i = ax_i + b $$

ê·¸ë ‡ë‹¤ë©´ corresponding linear transformationì„ weight, biasì— ì ìš©í•´ì•¼ ê¸°ì¡´ê³¼ ê°™ì€ í•™ìŠµì´ ëœë‹¤. ë°”ë€ weightì™€ bias í‘œí˜„ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$ w_{ji} \rightarrow \tilde{w}_{ji} = {1 \over a} w_{ji} \quad and \quad w_{j0} = w_{j0} - {b \over a} \sum_{i}{w_{ji}} $$

2. outputì— linear transformì„ ì ìš©

ë§Œì•½ output dataì— linear transformì„ ì ìš©í–ˆë‹¤ë©´ outputì€ ë‹¤ìŒê³¼ ê°™ì€ í˜•íƒœê°€ ëœë‹¤.

$$ y_k = \tilde{y}_{k} = cy_k + d $$

ì´ëŸ° linear transformationì´ ì ìš©ëœ outputì„ ì¶œë ¥í•˜ê¸° ìœ„í•´, ì´ì „ layerì¸ second layerì˜ weightì™€ biasëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

$$ w_{kj} \rightarrow \tilde{w}_{kj} cw_{kj} \quad and \quad w_{k0} = cw_{k0} + d $$

ì´ë ‡ê²Œ ë°”ë€ weightì—ëŠ” ê¸°ì¡´ weight decayë¥¼ ì ìš©í•  ìˆ˜ ì—†ë‹¤. ê¸°ì¡´ ì‹ì„ íšŒìƒí•´ ë³´ì.

$$ E(w) = E_0(w) + {1 \over 2}\lambda\sum_{i}{w_{i}^2} $$

ì´ ì‹ì€ ë°”ë€ weightì˜ ì„±ë¶„ë“¤ì„ ë°˜ì˜í•˜ì§€ ëª»í•œë‹¤. ë˜í•œ ì—¬ëŸ¬ ë‹¨ê³„ì—ì„œ scalingì„ ì ìš©í–ˆë‹¤ë©´, ë°”ë€ weightë“¤ì„ ì„œë¡œ ë‹¤ë¥´ê²Œ ì·¨ê¸‰í•´ì•¼ í•œë‹¤.

ë§Œì•½ ìœ„ ì˜ˆì‹œì—ì„œ input, output ëª¨ë‘ linear transformì„ ì ìš©í–ˆë‹¤ë©´ regularizationì„ ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜ì •í•˜ë©´ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

$$ {{\lambda}_1 \over {2}} \sum_{w \in W_1}{w}^2 + {{\lambda}_2 \over {2}} \sum_{w \in W_2}{w}^2 $$

- $ w_1 $ : first layerì˜ weights

- $ w_2 $ : second layerì˜ weights

ì´ ê²½ìš° rescaledëœ weightë¡œ ë°˜ì˜í•  ìˆ˜ ìˆë‹¤.

$$ \lambda_1 \rightarrow a^{1/2}\lambda_1 \quad and \quad \lambda_2 \rightarrow c^{-1/2}\lambda_2 $$

2. **ìƒˆë¡œìš´ ì˜¤ë¶„ë¥˜ë¥¼ ë§ê°**

ë˜í•œ ëŒ€ì²´ë¡œ weight decayëŠ” <U>ë‹¨ì¸µ í¼ì…‰íŠ¸ë¡ ì—ì„œë§Œ ì“°ì¸ë‹¤.</U> ê·¸ ì´ìœ ëŠ” ìƒˆë¡­ê²Œ ì˜¤ë¶„ë¥˜ëœ í›ˆë ¨ì ë“¤ì´ ê°€ì¤‘ì¹˜ ë²¡í„°ì— ë„ˆë¬´ í° ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ê²½ìš°, weight decayì— ì˜í•´ ë§ê°ì´ ë„ˆë¬´ ë¹¨ë¦¬ ì¼ì–´ë‚˜ëŠ” ê²½í–¥ì´ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ê²Œë‹¤ê°€ ë‹¨ì¸µ í¼ì…‰íŠ¸ë¡ ì€ ì´í›„ ì •ë¦¬í•  ë‹¤ë¥¸ ì¢…ë¥˜ì˜ regularization ê¸°ë²•ì´ ë” í”íˆ ì“°ì¸ë‹¤.

---

## 1.4.1.2 ì‹ ê²½ë§ êµ¬ì¡°ì™€ ë§¤ê°œë³€ìˆ˜ ê³µìœ 

**RNN**(Recurrent Neural Network, ìˆœí™˜ ì‹ ê²½ë§), **CNN**ì´ ëŒ€í‘œì ì´ë‹¤. í•œ ë¬¸ì¥ì„ êµ¬ì„±í•˜ëŠ” ì¼ë ¨ì˜ ë‹¨ì–´ë“¤ì´ ì„œë¡œ ì—°ê´€ë˜ì–´ ìˆì„ ë•Œê°€ ë§ê³ , ì´ë¯¸ì§€ì˜ ì¸ì ‘ í”½ì…€ë“¤ë„ ë§ˆì°¬ê°€ì§€ë¡œ ì—°ê´€ë˜ëŠ” ê²½ìš°ê°€ ì¼ë°˜ì ì´ë‹¤. ë”°ë¼ì„œ ì´ëŸ° í†µì°°ì„ ì´ìš©í•˜ë©´ ë” ì ì€ ìˆ˜ì˜ ë§¤ê°œë³€ìˆ˜ë¡œ ì‹ ê²½ë§ êµ¬ì¡°ë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤.

---

## 1.4.1.3 Ealry Stopping(ì¡°ê¸° ì¢…ë£Œ)

íŠ¹ì • ì¡°ê±´ì„ ë§Œì¡±í•˜ë©´ gradient descentì˜ ë°˜ë³µì„ ì¼ì° ëë‚´ëŠ” ê¸°ë²•ì´ë‹¤. ì¢…ë£Œ ì‹œì ì„ ê²°ì •í•˜ëŠ” ì¼ë°˜ì ì¸ ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

- ìš°ì„  train data setì˜ ì¼ë¶€ë¥¼ ë”°ë¡œ ë¹¼ì„œ validation data setì„ ë§Œë“ ë‹¤. 
 
- validation data setì„ ì‹œí—˜ ìë£Œë¡œ ì¨ì„œ modelì˜ errorë¥¼ ì¸¡ì •í•œë‹¤.
 
- validation dataì˜ errorê°€ ê°€ì¥ ì‘ì€ ì§€ì ì—ì„œ trainingì„ ëë‚¸ë‹¤.

![early stopping](images/early_stopping.png)

> weight vectorëŠ” ì›ì ì—ì„œ ì‹œì‘í•´ì„œ $w_{ML}$ ë¡œ ë‚˜ì•„ê°„ë‹¤.

Ealry Stoppingì€ regularizationì˜ ëŒ€ì•ˆì¸ë°, <U>ì§€ì†ë˜ëŠ” trainingìœ¼ë¡œ ê³„ì†í•´ì„œ ë§¤ê°œë³€ìˆ˜ê°€ ì»¤ì§€ëŠ” ê²ƒì„ ë°©ì§€í•œë‹¤</U>ëŠ” ì ì—ì„œ í•˜ë‚˜ì˜ regularization í•­ìœ¼ë¡œ ì‘ìš©í•œë‹¤ê³  í•  ìˆ˜ ìˆë‹¤.

---

## 1.4.1.4 ë„ˆë¹„ì™€ ê¹Šì´ì˜ ì ˆì¶©

hidden layer ì•ˆì— ìˆ˜ë§ì€ hidden unitì´ ìˆë‹¤ë©´ layerê°€ ë‘ ê°œì¸ ë‹¤ì¸µ ì‹ ê²½ë§ë„ ë³´í¸ì  í•¨ìˆ˜ ê·¼ì‚¬ê¸°ê°€ ë  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ ëŒ€ì²´ë¡œ depthë¥¼ ëŠ˜ë¦¬ë©´ì„œ hidden unitë“¤ì„ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ ì„¤ê³„í•˜ëŠ”ë°, ì´ëŸ° ì„¤ê³„ëŠ” ì¼ì¢…ì˜ regularizationì— í•´ë‹¹í•œë‹¤. depthë¥¼ ê³„ì† ê¹Šê²Œ ë§Œë“¤ë©´ì„œ unitì„ ì¤„ì´ë‹¤ ë³´ë©´, layerë¥¼ ëŠ˜ë ¤ì„œ ìƒê¸´ ë§¤ê°œë³€ìˆ˜ ì¦ê°€ë³´ë‹¤ë„ <U>layer ë„ˆë¹„ê°€ ì¤„ì–´ë“¤ë©´ì„œ ë§¤ê°œë³€ìˆ˜ê°€ ê°ì†Œí•œ ì˜í–¥ì´ ë” ì»¤ì§€ê²Œ ëœë‹¤</U>. 

ì´ ê²½ìš° overfittingë³´ë‹¤ëŠ” ë‚«ì§€ë§Œ, ë˜ ë‹¤ë¥¸ ë°©í–¥ì˜ ë¬¸ì œì ì„ í•´ê²°í•´ì•¼ ë  ìˆ˜ ìˆë‹¤. ì‹ ê²½ë§ì˜ ì—¬ëŸ¬ layerì—ì„œ loss functionì˜ ë¯¸ë¶„ê°’ ë³€ë™ì´ í¬ë©´, ì ì ˆí•œ ê°±ì‹ ì´ ì´ë¤„ì§€ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤. ê·¸ëŸ° ê²½í–¥ì€ **gradient vanishing**(ê¸°ìš¸ê¸° ì†Œì‹¤)ê³¼ **gradient explosion**(ê¸°ìš¸ê¸° í­ë°œ) í˜„ìƒì„ ì¼ìœ¼í‚¬ ìˆ˜ ìˆë‹¤.

> ê°„ë‹¨íˆ vanishing gradient problem(ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œ)ì€ backpropagationì´ ì§„í–‰ë˜ë©´ì„œ input ê·¼ì²˜ layerì˜ ê°±ì‹  í¬ê¸°ê°€ outputì— ê°€ê¹Œìš´ layerë“¤ì— ë¹„í•´ í›¨ì”¬ ì‘ì•„ì§€ëŠ” ë¬¸ì œë‹¤. 

> ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ê¸°ìš¸ê¸°ê°€ ë” í° activation functionì„ ì‚¬ìš©í•˜ê³  weightì˜ í¬ê¸° ìì²´ë„ í¬ê²Œ ì„¤ì •í•  ìˆ˜ ìˆì§€ë§Œ, ì •ë„ê°€ ì‹¬í•˜ë©´ ë°˜ëŒ€ë¡œ ê¸°ìš¸ê¸°ê°€ í­ë°œí•˜ëŠ” ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤.

---

## 1.4.1.5 ensemble method(ì•™ìƒë¸” ë°©ë²•)

modelì˜ generalization ëŠ¥ë ¥ì„ í‚¤ìš°ê¸° ìœ„í•´ **bagging**(ë°°ê¹…)ê³¼ ê°™ì€ ë‹¤ì–‘í•œ ì•™ìƒë¸” ë°©ë²•ì´ ì“°ì¸ë‹¤.(ì´ëŸ° ë°©ë²•ì€ ì‹ ê²½ë§ë¿ë§Œ ì•„ë‹ˆë¼ ëª¨ë“  ì¢…ë¥˜ì˜ ê¸°ê³„ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì— ì ìš©ëœë‹¤.) 

> Baggingì€ Bootstrap Aggregationì˜ ì•½ìë‹¤. sampleì„ ì—¬ëŸ¬ ë²ˆ ë½‘ì•„(Bootstrap) ê° ëª¨ë¸ì„ í•™ìŠµì‹œì¼œ ê²°ê³¼ë¬¼ì„ ì§‘ê³„(Aggregation)í•˜ëŠ” ë°©ë²•ì´ë‹¤.

ì‹ ê²½ë§ì— íŠ¹í™”ëœ ì•™ìƒë¸” ë°©ë²•ë„ ì—¬ëŸ¿ ìˆëŠ”ë°, ëŒ€í‘œì ìœ¼ë¡œ **dropout**(ë“œë¡­ì•„ì›ƒ), **dropconnect**(ë“œë¡­ì»¤ë„¥íŠ¸)ê°€ ìˆë‹¤. ëŒ€ì²´ë¡œ accuracyë¥¼ ì•½ 2% ì¦ê°€ì‹œí‚¤ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤. ê·¸ëŸ¬ë‚˜ êµ¬ì²´ì ì¸ ê°œì„  ì •ë„ëŠ” ì–´ë–¤ ìë£Œì¸ê°€, ì–´ë–¤ training ë°©ë²•ì¸ê°€ì— ë‹¬ë ¸ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ hidden layerì˜ activationì„ normalization(ì •ê·œí™”)í•´ ë²„ë¦¬ë©´ dropoutì˜ íš¨ê³¼ê°€ ì¤„ì–´ë“¤ ìˆ˜ ìˆë‹¤.

---
